<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.1 解釈可能性の重要性 | Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2.1 解釈可能性の重要性 | Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.1 解釈可能性の重要性 | Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2020-12-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="interpretability.html"/>
<link rel="next" href="解釈可能な手法の分類.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>要約</a></li>
<li class="chapter" data-level="" data-path="著者による序文.html"><a href="著者による序文.html"><i class="fa fa-check"></i>著者による序文</a></li>
<li class="chapter" data-level="" data-path="preface-by-the-translator.html"><a href="preface-by-the-translator.html"><i class="fa fa-check"></i>Preface by the Translator</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> イントロダクション</a><ul>
<li class="chapter" data-level="1.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>1.1</b> 物語の時間</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#信用失墜"><i class="fa fa-check"></i>信用失墜</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#フェルミのペーパークリップ"><i class="fa fa-check"></i>フェルミのペーパー・クリップ</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="機械学習とは何か.html"><a href="機械学習とは何か.html"><i class="fa fa-check"></i><b>1.2</b> 機械学習とは何か？</a></li>
<li class="chapter" data-level="1.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>1.3</b> 専門用語</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> 解釈可能性</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> 解釈可能性の重要性</a></li>
<li class="chapter" data-level="2.2" data-path="解釈可能な手法の分類.html"><a href="解釈可能な手法の分類.html"><i class="fa fa-check"></i><b>2.2</b> 解釈可能な手法の分類</a></li>
<li class="chapter" data-level="2.3" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html"><i class="fa fa-check"></i><b>2.3</b> 解釈可能性の範囲</a><ul>
<li class="chapter" data-level="2.3.1" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#全体的なモデルの解釈可能性"><i class="fa fa-check"></i><b>2.3.1</b> 全体的なモデルの解釈可能性</a></li>
<li class="chapter" data-level="2.3.2" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#モジュールレベルのモデルの全体的な解釈可能性"><i class="fa fa-check"></i><b>2.3.2</b> モジュールレベルのモデルの全体的な解釈可能性</a></li>
<li class="chapter" data-level="2.3.3" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#単一の予測に対する局所的な解釈"><i class="fa fa-check"></i><b>2.3.3</b> 単一の予測に対する局所的な解釈</a></li>
<li class="chapter" data-level="2.3.4" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#予測のグループに対する局所的な解釈"><i class="fa fa-check"></i><b>2.3.4</b> 予測のグループに対する局所的な解釈</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="解釈可能性の評価.html"><a href="解釈可能性の評価.html"><i class="fa fa-check"></i><b>2.4</b> 解釈可能性の評価</a></li>
<li class="chapter" data-level="2.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>2.5</b> 説明に関する性質</a></li>
<li class="chapter" data-level="2.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>2.6</b> 人間に優しい説明</a><ul>
<li class="chapter" data-level="2.6.1" data-path="explanation.html"><a href="explanation.html#説明とはなにか"><i class="fa fa-check"></i><b>2.6.1</b> 説明とはなにか</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> データセット</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> 自転車レンタル (回帰)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> YouTube スパムコメント (テキスト分類)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>3.3</b> 子宮頸がんのリスク要因(クラス分類)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>4</b> Interpretable Models</a><ul>
<li class="chapter" data-level="4.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>4.1</b> Linear Regression</a><ul>
<li class="chapter" data-level="4.1.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>4.1.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.1.2" data-path="limo.html"><a href="limo.html#example"><i class="fa fa-check"></i><b>4.1.2</b> Example</a></li>
<li class="chapter" data-level="4.1.3" data-path="limo.html"><a href="limo.html#visual-interpretation"><i class="fa fa-check"></i><b>4.1.3</b> Visual Interpretation</a></li>
<li class="chapter" data-level="4.1.4" data-path="limo.html"><a href="limo.html#explain-individual-predictions"><i class="fa fa-check"></i><b>4.1.4</b> Explain Individual Predictions</a></li>
<li class="chapter" data-level="4.1.5" data-path="limo.html"><a href="limo.html#cat-code"><i class="fa fa-check"></i><b>4.1.5</b> Encoding of Categorical Features</a></li>
<li class="chapter" data-level="4.1.6" data-path="limo.html"><a href="limo.html#do-linear-models-create-good-explanations"><i class="fa fa-check"></i><b>4.1.6</b> Do Linear Models Create Good Explanations?</a></li>
<li class="chapter" data-level="4.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>4.1.7</b> Sparse Linear Models</a></li>
<li class="chapter" data-level="4.1.8" data-path="limo.html"><a href="limo.html#advantages"><i class="fa fa-check"></i><b>4.1.8</b> Advantages</a></li>
<li class="chapter" data-level="4.1.9" data-path="limo.html"><a href="limo.html#disadvantages"><i class="fa fa-check"></i><b>4.1.9</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>4.2</b> ロジスティック回帰</a><ul>
<li class="chapter" data-level="4.2.1" data-path="logistic.html"><a href="logistic.html#理論"><i class="fa fa-check"></i><b>4.2.1</b> 理論</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="解釈性.html"><a href="解釈性.html"><i class="fa fa-check"></i><b>4.3</b> 解釈性</a><ul>
<li class="chapter" data-level="4.3.1" data-path="解釈性.html"><a href="解釈性.html#例"><i class="fa fa-check"></i><b>4.3.1</b> 例</a></li>
<li class="chapter" data-level="4.3.2" data-path="解釈性.html"><a href="解釈性.html#長所と短所"><i class="fa fa-check"></i><b>4.3.2</b> 長所と短所</a></li>
<li class="chapter" data-level="4.3.3" data-path="解釈性.html"><a href="解釈性.html#ソフトウェア"><i class="fa fa-check"></i><b>4.3.3</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>4.4</b> GLM, GAM and more</a><ul>
<li class="chapter" data-level="4.4.1" data-path="extend-lm.html"><a href="extend-lm.html#glm"><i class="fa fa-check"></i><b>4.4.1</b> Non-Gaussian Outcomes - GLMs</a></li>
<li class="chapter" data-level="4.4.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>4.4.2</b> Interactions</a></li>
<li class="chapter" data-level="4.4.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>4.4.3</b> Nonlinear Effects - GAMs</a></li>
<li class="chapter" data-level="4.4.4" data-path="extend-lm.html"><a href="extend-lm.html#advantages-1"><i class="fa fa-check"></i><b>4.4.4</b> Advantages</a></li>
<li class="chapter" data-level="4.4.5" data-path="extend-lm.html"><a href="extend-lm.html#disadvantages-1"><i class="fa fa-check"></i><b>4.4.5</b> Disadvantages</a></li>
<li class="chapter" data-level="4.4.6" data-path="extend-lm.html"><a href="extend-lm.html#software"><i class="fa fa-check"></i><b>4.4.6</b> Software</a></li>
<li class="chapter" data-level="4.4.7" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>4.4.7</b> Further Extensions</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>4.5</b> 決定木</a><ul>
<li class="chapter" data-level="4.5.1" data-path="tree.html"><a href="tree.html#決定木の解釈"><i class="fa fa-check"></i><b>4.5.1</b> 決定木の解釈</a></li>
<li class="chapter" data-level="4.5.2" data-path="tree.html"><a href="tree.html#例-1"><i class="fa fa-check"></i><b>4.5.2</b> 例</a></li>
<li class="chapter" data-level="4.5.3" data-path="tree.html"><a href="tree.html#長所"><i class="fa fa-check"></i><b>4.5.3</b> 長所</a></li>
<li class="chapter" data-level="4.5.4" data-path="tree.html"><a href="tree.html#短所"><i class="fa fa-check"></i><b>4.5.4</b> 短所</a></li>
<li class="chapter" data-level="4.5.5" data-path="tree.html"><a href="tree.html#ソフトウェア-1"><i class="fa fa-check"></i><b>4.5.5</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>4.6</b> Decision Rules</a><ul>
<li class="chapter" data-level="4.6.1" data-path="rules.html"><a href="rules.html#learn-rules-from-a-single-feature-oner"><i class="fa fa-check"></i><b>4.6.1</b> Learn Rules from a Single Feature (OneR)</a></li>
<li class="chapter" data-level="4.6.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>4.6.2</b> Sequential Covering</a></li>
<li class="chapter" data-level="4.6.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>4.6.3</b> Bayesian Rule Lists</a></li>
<li class="chapter" data-level="4.6.4" data-path="rules.html"><a href="rules.html#advantages-2"><i class="fa fa-check"></i><b>4.6.4</b> Advantages</a></li>
<li class="chapter" data-level="4.6.5" data-path="rules.html"><a href="rules.html#disadvantages-2"><i class="fa fa-check"></i><b>4.6.5</b> Disadvantages</a></li>
<li class="chapter" data-level="4.6.6" data-path="rules.html"><a href="rules.html#software-and-alternatives"><i class="fa fa-check"></i><b>4.6.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>4.7</b> RuleFit</a><ul>
<li class="chapter" data-level="4.7.1" data-path="rulefit.html"><a href="rulefit.html#interpretation-and-example"><i class="fa fa-check"></i><b>4.7.1</b> Interpretation and Example</a></li>
<li class="chapter" data-level="4.7.2" data-path="rulefit.html"><a href="rulefit.html#theory"><i class="fa fa-check"></i><b>4.7.2</b> Theory</a></li>
<li class="chapter" data-level="4.7.3" data-path="rulefit.html"><a href="rulefit.html#advantages-3"><i class="fa fa-check"></i><b>4.7.3</b> Advantages</a></li>
<li class="chapter" data-level="4.7.4" data-path="rulefit.html"><a href="rulefit.html#disadvantages-3"><i class="fa fa-check"></i><b>4.7.4</b> Disadvantages</a></li>
<li class="chapter" data-level="4.7.5" data-path="rulefit.html"><a href="rulefit.html#software-and-alternative"><i class="fa fa-check"></i><b>4.7.5</b> Software and Alternative</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>4.8</b> Other Interpretable Models</a><ul>
<li class="chapter" data-level="4.8.1" data-path="other-interpretable.html"><a href="other-interpretable.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>4.8.1</b> Naive Bayes Classifier</a></li>
<li class="chapter" data-level="4.8.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>4.8.2</b> K-Nearest Neighbors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>5</b> Model-Agnostic Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>5.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="5.1.1" data-path="pdp.html"><a href="pdp.html#例-2"><i class="fa fa-check"></i><b>5.1.1</b> 例</a></li>
<li class="chapter" data-level="5.1.2" data-path="pdp.html"><a href="pdp.html#長所-1"><i class="fa fa-check"></i><b>5.1.2</b> 長所</a></li>
<li class="chapter" data-level="5.1.3" data-path="pdp.html"><a href="pdp.html#短所-1"><i class="fa fa-check"></i><b>5.1.3</b> 短所</a></li>
<li class="chapter" data-level="5.1.4" data-path="pdp.html"><a href="pdp.html#ソフトウェアと代替手法"><i class="fa fa-check"></i><b>5.1.4</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>5.2</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ice.html"><a href="ice.html#例-3"><i class="fa fa-check"></i><b>5.2.1</b> 例</a></li>
<li class="chapter" data-level="5.2.2" data-path="ice.html"><a href="ice.html#長所-2"><i class="fa fa-check"></i><b>5.2.2</b> 長所</a></li>
<li class="chapter" data-level="5.2.3" data-path="ice.html"><a href="ice.html#短所-2"><i class="fa fa-check"></i><b>5.2.3</b> 短所</a></li>
<li class="chapter" data-level="5.2.4" data-path="ice.html"><a href="ice.html#ソフトウェアと代替手法-1"><i class="fa fa-check"></i><b>5.2.4</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>5.3</b> Accumulated Local Effects (ALE) Plot</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ale.html"><a href="ale.html#motivation-and-intuition"><i class="fa fa-check"></i><b>5.3.1</b> Motivation and Intuition</a></li>
<li class="chapter" data-level="5.3.2" data-path="ale.html"><a href="ale.html#theory-1"><i class="fa fa-check"></i><b>5.3.2</b> Theory</a></li>
<li class="chapter" data-level="5.3.3" data-path="ale.html"><a href="ale.html#estimation"><i class="fa fa-check"></i><b>5.3.3</b> Estimation</a></li>
<li class="chapter" data-level="5.3.4" data-path="ale.html"><a href="ale.html#examples"><i class="fa fa-check"></i><b>5.3.4</b> Examples</a></li>
<li class="chapter" data-level="5.3.5" data-path="ale.html"><a href="ale.html#advantages-4"><i class="fa fa-check"></i><b>5.3.5</b> Advantages</a></li>
<li class="chapter" data-level="5.3.6" data-path="ale.html"><a href="ale.html#disadvantages-4"><i class="fa fa-check"></i><b>5.3.6</b> Disadvantages</a></li>
<li class="chapter" data-level="5.3.7" data-path="ale.html"><a href="ale.html#implementation-and-alternatives"><i class="fa fa-check"></i><b>5.3.7</b> Implementation and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>5.4</b> Feature Interaction</a><ul>
<li class="chapter" data-level="5.4.1" data-path="interaction.html"><a href="interaction.html#feature-interaction"><i class="fa fa-check"></i><b>5.4.1</b> Feature Interaction?</a></li>
<li class="chapter" data-level="5.4.2" data-path="interaction.html"><a href="interaction.html#theory-friedmans-h-statistic"><i class="fa fa-check"></i><b>5.4.2</b> Theory: Friedman's H-statistic</a></li>
<li class="chapter" data-level="5.4.3" data-path="interaction.html"><a href="interaction.html#examples-1"><i class="fa fa-check"></i><b>5.4.3</b> Examples</a></li>
<li class="chapter" data-level="5.4.4" data-path="interaction.html"><a href="interaction.html#advantages-5"><i class="fa fa-check"></i><b>5.4.4</b> Advantages</a></li>
<li class="chapter" data-level="5.4.5" data-path="interaction.html"><a href="interaction.html#disadvantages-5"><i class="fa fa-check"></i><b>5.4.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.4.6" data-path="interaction.html"><a href="interaction.html#implementations"><i class="fa fa-check"></i><b>5.4.6</b> Implementations</a></li>
<li class="chapter" data-level="5.4.7" data-path="interaction.html"><a href="interaction.html#alternatives"><i class="fa fa-check"></i><b>5.4.7</b> Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>5.5</b> Permutation Feature Importance</a><ul>
<li class="chapter" data-level="5.5.1" data-path="feature-importance.html"><a href="feature-importance.html#theory-2"><i class="fa fa-check"></i><b>5.5.1</b> Theory</a></li>
<li class="chapter" data-level="5.5.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>5.5.2</b> Should I Compute Importance on Training or Test Data?</a></li>
<li class="chapter" data-level="5.5.3" data-path="feature-importance.html"><a href="feature-importance.html#example-and-interpretation"><i class="fa fa-check"></i><b>5.5.3</b> Example and Interpretation</a></li>
<li class="chapter" data-level="5.5.4" data-path="feature-importance.html"><a href="feature-importance.html#advantages-6"><i class="fa fa-check"></i><b>5.5.4</b> Advantages</a></li>
<li class="chapter" data-level="5.5.5" data-path="feature-importance.html"><a href="feature-importance.html#disadvantages-6"><i class="fa fa-check"></i><b>5.5.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.5.6" data-path="feature-importance.html"><a href="feature-importance.html#software-and-alternatives-1"><i class="fa fa-check"></i><b>5.5.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>5.6</b> Global Surrogate</a><ul>
<li class="chapter" data-level="5.6.1" data-path="global.html"><a href="global.html#theory-3"><i class="fa fa-check"></i><b>5.6.1</b> Theory</a></li>
<li class="chapter" data-level="5.6.2" data-path="global.html"><a href="global.html#example-1"><i class="fa fa-check"></i><b>5.6.2</b> Example</a></li>
<li class="chapter" data-level="5.6.3" data-path="global.html"><a href="global.html#advantages-7"><i class="fa fa-check"></i><b>5.6.3</b> Advantages</a></li>
<li class="chapter" data-level="5.6.4" data-path="global.html"><a href="global.html#disadvantages-7"><i class="fa fa-check"></i><b>5.6.4</b> Disadvantages</a></li>
<li class="chapter" data-level="5.6.5" data-path="global.html"><a href="global.html#software-1"><i class="fa fa-check"></i><b>5.6.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>5.7</b> Local Surrogate (LIME)</a><ul>
<li class="chapter" data-level="5.7.1" data-path="lime.html"><a href="lime.html#lime-for-tabular-data"><i class="fa fa-check"></i><b>5.7.1</b> LIME for Tabular Data</a></li>
<li class="chapter" data-level="5.7.2" data-path="lime.html"><a href="lime.html#lime-for-text"><i class="fa fa-check"></i><b>5.7.2</b> LIME for Text</a></li>
<li class="chapter" data-level="5.7.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>5.7.3</b> LIME for Images</a></li>
<li class="chapter" data-level="5.7.4" data-path="lime.html"><a href="lime.html#advantages-8"><i class="fa fa-check"></i><b>5.7.4</b> Advantages</a></li>
<li class="chapter" data-level="5.7.5" data-path="lime.html"><a href="lime.html#disadvantages-8"><i class="fa fa-check"></i><b>5.7.5</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="anchors.html"><a href="anchors.html"><i class="fa fa-check"></i><b>5.8</b> Scoped Rules (Anchors)</a><ul>
<li class="chapter" data-level="5.8.1" data-path="anchors.html"><a href="anchors.html#finding-anchors"><i class="fa fa-check"></i><b>5.8.1</b> Finding Anchors</a></li>
<li class="chapter" data-level="5.8.2" data-path="anchors.html"><a href="anchors.html#complexity-and-runtime"><i class="fa fa-check"></i><b>5.8.2</b> Complexity and Runtime</a></li>
<li class="chapter" data-level="5.8.3" data-path="anchors.html"><a href="anchors.html#tabular-data-example"><i class="fa fa-check"></i><b>5.8.3</b> Tabular Data Example</a></li>
<li class="chapter" data-level="5.8.4" data-path="anchors.html"><a href="anchors.html#advantages-9"><i class="fa fa-check"></i><b>5.8.4</b> Advantages</a></li>
<li class="chapter" data-level="5.8.5" data-path="anchors.html"><a href="anchors.html#disadvantages-9"><i class="fa fa-check"></i><b>5.8.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.8.6" data-path="anchors.html"><a href="anchors.html#software-and-alternatives-2"><i class="fa fa-check"></i><b>5.8.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>5.9</b> Shapley Values</a><ul>
<li class="chapter" data-level="5.9.1" data-path="shapley.html"><a href="shapley.html#general-idea"><i class="fa fa-check"></i><b>5.9.1</b> General Idea</a></li>
<li class="chapter" data-level="5.9.2" data-path="shapley.html"><a href="shapley.html#examples-and-interpretation"><i class="fa fa-check"></i><b>5.9.2</b> Examples and Interpretation</a></li>
<li class="chapter" data-level="5.9.3" data-path="shapley.html"><a href="shapley.html#the-shapley-value-in-detail"><i class="fa fa-check"></i><b>5.9.3</b> The Shapley Value in Detail</a></li>
<li class="chapter" data-level="5.9.4" data-path="shapley.html"><a href="shapley.html#advantages-10"><i class="fa fa-check"></i><b>5.9.4</b> Advantages</a></li>
<li class="chapter" data-level="5.9.5" data-path="shapley.html"><a href="shapley.html#disadvantages-10"><i class="fa fa-check"></i><b>5.9.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.9.6" data-path="shapley.html"><a href="shapley.html#software-and-alternatives-3"><i class="fa fa-check"></i><b>5.9.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="shap.html"><a href="shap.html"><i class="fa fa-check"></i><b>5.10</b> SHAP (SHapley Additive exPlanations)</a><ul>
<li class="chapter" data-level="5.10.1" data-path="shap.html"><a href="shap.html#definition"><i class="fa fa-check"></i><b>5.10.1</b> Definition</a></li>
<li class="chapter" data-level="5.10.2" data-path="shap.html"><a href="shap.html#kernelshap"><i class="fa fa-check"></i><b>5.10.2</b> KernelSHAP</a></li>
<li class="chapter" data-level="5.10.3" data-path="shap.html"><a href="shap.html#treeshap"><i class="fa fa-check"></i><b>5.10.3</b> TreeSHAP</a></li>
<li class="chapter" data-level="5.10.4" data-path="shap.html"><a href="shap.html#examples-2"><i class="fa fa-check"></i><b>5.10.4</b> Examples</a></li>
<li class="chapter" data-level="5.10.5" data-path="shap.html"><a href="shap.html#shap-feature-importance"><i class="fa fa-check"></i><b>5.10.5</b> SHAP Feature Importance</a></li>
<li class="chapter" data-level="5.10.6" data-path="shap.html"><a href="shap.html#shap-summary-plot"><i class="fa fa-check"></i><b>5.10.6</b> SHAP Summary Plot</a></li>
<li class="chapter" data-level="5.10.7" data-path="shap.html"><a href="shap.html#shap-dependence-plot"><i class="fa fa-check"></i><b>5.10.7</b> SHAP Dependence Plot</a></li>
<li class="chapter" data-level="5.10.8" data-path="shap.html"><a href="shap.html#shap-interaction-values"><i class="fa fa-check"></i><b>5.10.8</b> SHAP Interaction Values</a></li>
<li class="chapter" data-level="5.10.9" data-path="shap.html"><a href="shap.html#clustering-shap-values"><i class="fa fa-check"></i><b>5.10.9</b> Clustering SHAP values</a></li>
<li class="chapter" data-level="5.10.10" data-path="shap.html"><a href="shap.html#advantages-11"><i class="fa fa-check"></i><b>5.10.10</b> Advantages</a></li>
<li class="chapter" data-level="5.10.11" data-path="shap.html"><a href="shap.html#disadvantages-11"><i class="fa fa-check"></i><b>5.10.11</b> Disadvantages</a></li>
<li class="chapter" data-level="5.10.12" data-path="shap.html"><a href="shap.html#software-2"><i class="fa fa-check"></i><b>5.10.12</b> Software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>6</b> Example-Based Explanations</a><ul>
<li class="chapter" data-level="6.1" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>6.1</b> Counterfactual Explanations</a><ul>
<li class="chapter" data-level="6.1.1" data-path="counterfactual.html"><a href="counterfactual.html#generating-counterfactual-explanations"><i class="fa fa-check"></i><b>6.1.1</b> Generating Counterfactual Explanations</a></li>
<li class="chapter" data-level="6.1.2" data-path="counterfactual.html"><a href="counterfactual.html#examples-3"><i class="fa fa-check"></i><b>6.1.2</b> Examples</a></li>
<li class="chapter" data-level="6.1.3" data-path="counterfactual.html"><a href="counterfactual.html#advantages-12"><i class="fa fa-check"></i><b>6.1.3</b> Advantages</a></li>
<li class="chapter" data-level="6.1.4" data-path="counterfactual.html"><a href="counterfactual.html#disadvantages-12"><i class="fa fa-check"></i><b>6.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="6.1.5" data-path="counterfactual.html"><a href="counterfactual.html#example-software"><i class="fa fa-check"></i><b>6.1.5</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>6.2</b> Adversarial Examples</a><ul>
<li class="chapter" data-level="6.2.1" data-path="adversarial.html"><a href="adversarial.html#methods-and-examples"><i class="fa fa-check"></i><b>6.2.1</b> Methods and Examples</a></li>
<li class="chapter" data-level="6.2.2" data-path="adversarial.html"><a href="adversarial.html#the-cybersecurity-perspective"><i class="fa fa-check"></i><b>6.2.2</b> The Cybersecurity Perspective</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>6.3</b> Prototypes and Criticisms</a><ul>
<li class="chapter" data-level="6.3.1" data-path="proto.html"><a href="proto.html#theory-4"><i class="fa fa-check"></i><b>6.3.1</b> Theory</a></li>
<li class="chapter" data-level="6.3.2" data-path="proto.html"><a href="proto.html#examples-4"><i class="fa fa-check"></i><b>6.3.2</b> Examples</a></li>
<li class="chapter" data-level="6.3.3" data-path="proto.html"><a href="proto.html#advantages-13"><i class="fa fa-check"></i><b>6.3.3</b> Advantages</a></li>
<li class="chapter" data-level="6.3.4" data-path="proto.html"><a href="proto.html#disadvantages-13"><i class="fa fa-check"></i><b>6.3.4</b> Disadvantages</a></li>
<li class="chapter" data-level="6.3.5" data-path="proto.html"><a href="proto.html#code-and-alternatives"><i class="fa fa-check"></i><b>6.3.5</b> Code and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>6.4</b> Influential Instances</a><ul>
<li class="chapter" data-level="6.4.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>6.4.1</b> Deletion Diagnostics</a></li>
<li class="chapter" data-level="6.4.2" data-path="influential.html"><a href="influential.html#influence-functions"><i class="fa fa-check"></i><b>6.4.2</b> Influence Functions</a></li>
<li class="chapter" data-level="6.4.3" data-path="influential.html"><a href="influential.html#advantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>6.4.3</b> Advantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="6.4.4" data-path="influential.html"><a href="influential.html#disadvantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>6.4.4</b> Disadvantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="6.4.5" data-path="influential.html"><a href="influential.html#software-and-alternatives-4"><i class="fa fa-check"></i><b>6.4.5</b> Software and Alternatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>7</b> Neural Network Interpretation</a><ul>
<li class="chapter" data-level="7.1" data-path="cnn-features.html"><a href="cnn-features.html"><i class="fa fa-check"></i><b>7.1</b> Learned Features</a><ul>
<li class="chapter" data-level="7.1.1" data-path="cnn-features.html"><a href="cnn-features.html#feature-visualization"><i class="fa fa-check"></i><b>7.1.1</b> Feature Visualization</a></li>
<li class="chapter" data-level="7.1.2" data-path="cnn-features.html"><a href="cnn-features.html#network-dissection"><i class="fa fa-check"></i><b>7.1.2</b> Network Dissection</a></li>
<li class="chapter" data-level="7.1.3" data-path="cnn-features.html"><a href="cnn-features.html#advantages-14"><i class="fa fa-check"></i><b>7.1.3</b> Advantages</a></li>
<li class="chapter" data-level="7.1.4" data-path="cnn-features.html"><a href="cnn-features.html#disadvantages-14"><i class="fa fa-check"></i><b>7.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="7.1.5" data-path="cnn-features.html"><a href="cnn-features.html#software-and-further-material"><i class="fa fa-check"></i><b>7.1.5</b> Software and Further Material</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>8</b> A Look into the Crystal Ball</a><ul>
<li class="chapter" data-level="8.1" data-path="the-future-of-machine-learning.html"><a href="the-future-of-machine-learning.html"><i class="fa fa-check"></i><b>8.1</b> The Future of Machine Learning</a></li>
<li class="chapter" data-level="8.2" data-path="the-future-of-interpretability.html"><a href="the-future-of-interpretability.html"><i class="fa fa-check"></i><b>8.2</b> The Future of Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>9</b> Contribute to the Book</a></li>
<li class="chapter" data-level="10" data-path="cite.html"><a href="cite.html"><i class="fa fa-check"></i><b>10</b> Citing this Book</a></li>
<li class="chapter" data-level="11" data-path="translations.html"><a href="translations.html"><i class="fa fa-check"></i><b>11</b> Translations</a></li>
<li class="chapter" data-level="12" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>12</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a><ul>
<li class="chapter" data-level="" data-path="r-packages-used-for-examples.html"><a href="r-packages-used-for-examples.html"><i class="fa fa-check"></i>R Packages Used for Examples</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="interpretability-importance" class="section level2">
<h2><span class="header-section-number">2.1</span> 解釈可能性の重要性</h2>
<p>If a machine learning model performs well, <strong>why do not we just trust the model</strong> and ignore <strong>why</strong> it made a certain decision? &quot;The problem is that a single metric, such as classification accuracy, is an incomplete description of most real-world tasks.&quot; (Doshi-Velez and Kim 2017 <a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>)</p>
<p>　機械学習のモデルがうまく動作している際、なぜ<strong>そのままモデルを信用</strong>し、<strong>なぜその決定がなされたか</strong>を無視してはいけないのでしょうか。 &quot;問題は、分類精度などの単一の評価方式では、多くの現実世界の問題を表現するには不完全なものであるということです。&quot; (Doshi-Velez and Kim 2017 <a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>)</p>
<!-- Let us dive deeper into the reasons why interpretability is so important.
When it comes to predictive modeling, you have to make a trade-off:
Do you just want to know **what** is predicted?
For example, the probability that a customer will churn or how effective some drug will be for a patient.
Or do you want to know **why** the prediction was made and possibly pay for the interpretability with a drop in predictive performance?
In some cases, you do not care why a decision was made, it is enough to know that the predictive performance on a test dataset was good.
But in other cases, knowing the 'why' can help you learn more about the problem, the data and the reason why a model might fail.
Some models may not require explanations  because they are used in a low-risk environment, meaning a mistake will not have serious consequences, (e.g. a movie recommender system) or the method has already been extensively studied and evaluated (e.g. optical character recognition).
The need for interpretability arises from an incompleteness in problem formalization (Doshi-Velez and Kim 2017), which means that for certain problems or tasks it is not enough to get the prediction (the **what**).
The model must also explain how it came to the prediction (the **why**), because a correct prediction only partially solves your original problem.
The following reasons drive the demand for interpretability and explanations (Doshi-Velez and Kim 2017 and Miller 2017). -->
<p>　なぜモデルの解釈可能性がそれほど重要なのか、もう少し考えてみましょう。何らかの予測モデルを構築する場合、そこに発生するトレードオフについて考える必要があります: あなたは顧客を集める可能性や、薬がどれだけ患者に効果的か、といったそのモデルが予測する<strong>結果</strong>についてのみ知りたいのでしょうか。それともたとえ予測性能が下がったとしても予測がなされた<strong>理由</strong>が知りたいのでしょうか。確かに、一部の事例では予測がなされた理由は必要なく、テストデータに対する予測性能のみを知ることができれば十分だと思います。しかし、この<strong>理由</strong>について知ることはその問題やデータに対する理解を深め、モデルが判断を誤る際にもその原因を探ることに役立ちます。また、映画のレコメンデーションといった失敗した際にの影響がそれほど大きくないリスクの低い環境で扱われる場合や、文字認識などといった、用いる手法がすでに広範にわたって研究され評価されている場合などでは、モデルの解釈はそれほで必要ではないでしょう。説明可能であることの必要性は、問題の不完全な定式化によって生じるのです(Doshi-Velez and Kim 2017)。これは、ある特定の問題や課題に対しては、予測（ <strong>結果</strong>）を得るだけでは不十分であるということです。このような場合は、正確な予測は本来の問題の一部のみを解決しただけであり、モデルが予測に至った経緯（ <strong>理由</strong>）も説明されるべきなのです。モデルの解釈・説明は次のような観点からも必要とされます（Doshi-Velez and Kim 2017 and Miller 2017）。</p>
<!-- **Human curiosity and learning**: Humans have a mental model of their environment that is updated when something unexpected happens.
This update is performed by finding an explanation for the unexpected event.
For example, a human feels unexpectedly sick and asks, "Why do I feel so sick?".
He learns that he gets sick every time he eats those red berries.
He updates his mental model and decides that the berries caused the sickness and should therefore be avoided.
When opaque machine learning models are used in research, scientific findings remain completely hidden if the model only gives predictions without explanations.
To facilitate learning and satisfy curiosity as to why certain predictions or behaviors are created by machines, interpretability and explanations are crucial.
Of course, humans do not need explanations for everything that happens.
For most people it is okay that they do not understand how a computer works.
Unexpected events makes us curious.
For example: Why is my computer shutting down unexpectedly? -->
<p><strong>人間の好奇心と学び</strong>: 私たち人間は自分の周囲の環境に対する心理的なモデルを一人一人が持っており、何か予測していないことが起こった際、このモデルを更新しています。ただし、このモデルの更新はこれらの出来事の原因がわかって初めて行われます。例えば、もし、思いがけず病気にかかってしまったとすると、なぜ病気にかかってしまったのかを考えるでしょう。その後、赤い木の実を食べた後に毎回具合が悪くなることに気付きます。このとき赤い木の実が具合を悪くさせること、また今後赤い木の実は食べないようにするよう、あなたは自分のモデルを更新します。これに対し、不透明なモデルが研究に使用された場合、科学的な知見は全く得られません。問題に対する理解を深め、自身の好奇心を満たすためには、モデルの説明可能性や解釈可能性は不可欠なのです。もちろん、私たちは現実に起こりうる全ての物事に対し説明が必要であるわけではありません。多くの人はコンピュータが動く原理を理解しなくても全く問題ないでしょう。しかし、それでも予想外の出来事というのは私たちの好奇心をそそります。考えてみてください、なにが原因でコンピュータは突然シャットダウンしてしまうのでしょうか。</p>
<!-- Closely related to learning is the human desire to **find meaning in the world**.
We want to harmonize contradictions or inconsistencies between elements of our knowledge structures.
"Why did my dog bite me even though it has never done so before?" a human might ask.
There is a contradiction between the knowledge of the dog's past behavior and the newly made, unpleasant experience of the bite.
The vet's explanation reconciles the dog owner's contradiction:
"The dog was under stress and bit."
The more a machine's decision affects a person's life, the more important it is for the machine to explain its behavior.
If a machine learning model rejects a loan application, this may be completely unexpected for the applicants.
They can only reconcile this inconsistency between expectation and reality with some kind of explanation.
The explanations do not actually have to fully explain the situation, but should address a main cause.
Another example is algorithmic product recommendation.
Personally, I always think about why certain products or movies have been algorithmically recommended to me.
Often it is quite clear:
Advertising follows me on the Internet because I recently bought a washing machine, and I know that in the next days I will be followed by advertisements for washing machines.
Yes, it makes sense to suggest gloves if I already have a winter hat in my shopping cart.
The algorithm recommends this movie, because users who liked other movies I liked also enjoyed the recommended movie.
Increasingly, Internet companies are adding explanations to their recommendations.
A good example are product recommendations, which are based on frequently purchased product combinations: -->
<p>　学びに大きく関係しているのが、人間の<strong>世の中の意味を見出したい</strong>という願望です。私たちは出来事と自分の持っている知識との矛盾や不一致を調和させようとします。「なぜうちの犬はさっき自分のことを噛んできたのだろうか？今までそんなことはなかったのに。」このように考えるわけです。ここには飼い犬の過去の振る舞いと、つい先ほど噛まれたという不快な事実という矛盾があります。 獣医の説明は、次のように矛盾を解決します: 「その犬はストレスに晒されていたのでしょう。」 機械の判断が人間の生活により影響を及ぼすようになるのであれば、機械自身がその判断について説明できることがより重要となってきます。もし機械学習モデルがローンの申し込みを拒否するよう判断したならば、これは申し込んだ側にとって予想外の出来事でしょう。 この予想と実際の結果の矛盾を埋めるには何らかの説明が必要です。この説明では状況を完璧に説明する必要はないでしょうが、主要な原因については言及する必要があるでしょう。 別の例としては、アルゴリズムを元にした商品のレコメンデーションです。個人的な話ですが、私はある商品や映画がなぜアルゴリズム的に自分に推薦されたのか、いつも考えてしまいます。 多くの場合は明らかです。直近に洗濯機を購入していたとすると、次の数日間は洗濯機に関する広告が表示されるでしょう。冬の帽子をすでに買い物かごに入れている場合に手袋がおすすめされるのも納得がいきます。 また映画のレコメンデーション機能は他のユーザーが気に入った映画に基づいて映画を提案します。最近では、インターネット事業を手掛けるより多くの会社がおすすめ機能に説明を加えています。分かりやすい例としては、他ユーザーが合わせて購入する商品をもとにした商品のおすすめ機能が挙げられます。</p>
<!--
fig.cap=’Recommended products that are frequently bought together’
-->
<div class="figure"><span id="fig:amazon-recommendation"></span>
<img src="images/amazon-freq-bought-together.png" alt="よく一緒に購入されている商品" width="500" />
<p class="caption">
FIGURE 2.1: よく一緒に購入されている商品
</p>
</div>
<!-- In many scientific disciplines there is a change from qualitative to quantitative methods (e.g. sociology, psychology), and also towards machine learning (biology, genomics).  -- -->
<p>社会学や哲学など、多くの科学分野において分析の手法は定性的なものから定量的なものへと変化しています。中には生物学や遺伝学など、機械学習へと手法が移っている分野もあります。</p>
<!-- The **goal of science** is to gain knowledge, but many problems are solved with big datasets and black box machine learning models. -->
<p><strong>科学の目標</strong>は知識を得ることですが、多くの問題は大規模データとブラックボックスな機械学習モデルで解かれてしまっています。</p>
<!-- The model itself becomes the source of knowledge instead of the data.
Interpretability makes it possible to extract this additional knowledge captured by the model. -->
<p>モデルはそれ自身がデータの代わりに知識の源となります。 解釈可能性はこのモデルから得られる付加的な知識を抽出することを可能にします。</p>
<!-- Machine learning models take on real-world tasks that require **safety measures** and testing. -->
<p>機械学習モデルは、<strong>安全対策</strong>とテストが必要な実務課題を引き受けるようになります。</p>
<!-- Imagine a self-driving car automatically detects cyclists based on a deep learning system.
You want to be 100% sure that the abstraction the system has learned is error-free, because running over cyclists is quite bad. →
自動運転車がディープラーニングのシステムに基づいて自転車を自動検出する場面を想像してください。
車が自転車を轢くことは非常に危険であるため、そのシステムが学習した抽象概念には全く間違いが無いと100%確信できる必要があります。

<!-- An explanation might reveal that the most important learned feature is to recognize the two wheels of a bicycle, and this explanation helps you think about edge cases like bicycles with side bags that partially cover the wheels. -->
<p>モデルを読み解くと、学習によって得られた自転車の最も重要な特徴が2つの車輪を認識することだとしましょう。すると、このモデルによる説明から、車輪の一部を覆うサイドバッグをつけた自転車のようなコーナーケースを考えるのに役立ちます。</p>
<!-- By default, machine learning models pick up biases from the training data.
This can turn your machine learning models into racists that discriminate against protected groups.
Interpretability is a useful debugging tool for **detecting bias** in machine learning models. -->
<p>デフォルトでは、機械学習モデルは学習データに内在するバイアスを反映してしまいます。これは機械学習モデルを、保護されるべきグループを差別する人種差別主義者に変えうるものです。 解釈可能性は、機械学習モデルの<strong>バイアス検出</strong>のための便利なデバッグツールになります。</p>
<!-- It might happen that the machine learning model you have trained for automatic approval or rejection of credit applications discriminates against a minority.
Your main goal is to grant loans only to people who will eventually repay them. -->
<p>例えば、クレジット申請を自動で承認、拒否をするために学習したモデルが、少数派を差別するケースが考えられます。 企業の最大の目標はきちんと返済する人にだけローンを貸すことです。</p>
<!-- The incompleteness of the problem formulation in this case lies in the fact that you not only want to minimize loan defaults, but are also obliged not to discriminate on the basis of certain demographics.
This is an additional constraint that is part of your problem formulation (granting loans in a low-risk and compliant way) that is not covered by the loss function the machine learning model was optimized for. -->
<p>この例における問題の定式化の不備は、ローンの不履行を最小化するだけではなく、特定の人口統計に基づいて差別をしない義務があることです。 低リスクでなるべく多くの人を受け入れるようにローンを貸し出すという問題の定式化には、機械学習モデルが最適化する損失関数でカバーできていない追加の制約を加える必要があります。</p>
<!-- The process of integrating machines and algorithms into our daily lives requires interpretability to increase **social acceptance**.
People attribute beliefs, desires, intentions and so on to objects.
In a famous experiment, Heider and Simmel (1944) [^Heider] showed participants videos of shapes in which a circle opened a "door" to enter a "room" (which was simply a rectangle).
The participants described the actions of the shapes as they would describe the actions of a human agent, assigning intentions and even emotions and personality traits to the shapes. -->
<p>機械とアルゴリズムを私たちの日常生活に取り入れるプロセスとして、<strong>社会的受容</strong>を高めるための解釈可能性が必要です。 人間は、信念や欲求、意図などが物体にあると考えます。 有名な実験で、Heider and Simmel（1944）[^ Heider]は、円が「ドア」を開いて「部屋」（ただの長方形）に入るようなビデオを参加者に見せました。 参加者は、人間の行動を説明するのと同様に図形の行動を説明し、意図、さらには感情や性格までもを図形に割り当てました。</p>
<!-- Robots are a good example, like my vacuum cleaner, which I named "Doge". -->
<p>私が&quot;Doge&quot;と名付けたロボット掃除機のように、ロボットはその良い例です。 <!-- If Doge gets stuck, I think:
"Doge wants to keep cleaning, but asks me for help because it got stuck." --> 例えばもしDogeが停止していると、私はこう考えるでしょう。 「Dogeは掃除を続けたがっているが、動けなくなってしまったから私に助けを求めてきたぞ。」</p>
<!-- Later, when Doge finishes cleaning and searches the home base to recharge, I think:
"Doge has a desire to recharge and intends to find the home base." -->
<p>しばらくしてDogeが掃除を終え、充電台を探していれば私はこう考えます。 「Dogeは充電したがっていて、そのために台を見つけるつもりなんだ。」</p>
<!-- I also attribute personality traits:
"Doge is a bit dumb, but in a cute way." -->
<p>また、私はDogeに個性をも見出すでしょう。 「Dogeは少し抜けているが、そこが可愛いんだ。」</p>
<!-- These are my thoughts, especially when I find out that Doge has knocked over a plant while dutifully vacuuming the house. -->
<p>これらは私の考えですが、特にDogeが真面目に家を掃除しようとして、植物を倒してしまった時などにそう感じます。</p>
<!-- A machine or algorithm that explains its predictions will find more acceptance.
See also the [chapter on explanations](#explanation), which argues that explanations are a social process. -->
<p>予測の理由を説明できる機械やアルゴリズムはより広く受け入れられるでしょう。 説明は社会的プロセスであると主張している<a href="explanation.html#explanation">説明</a>についての章も参照してください。</p>
<!--Explanations are used to **manage social interactions**.-->
<p>説明は<strong>社会的相互作用を管理する</strong>のに使われます。</p>
<!-- By creating a shared meaning of something, the explainer influences the actions, emotions and beliefs of the recipient of the explanation. -->
<p>説明者は何かしらの意図を共有することで、説明を受ける人の行動や感情、信念に影響をもたらします。</p>
<!-- For a machine to interact with us, it may need to shape our emotions and beliefs.
Machines have to "persuade" us, so that they can achieve their intended goal. -->
<p>機械が私たちが互いに影響し合うためには、人間の感情や信念を形にする必要があるかもしれません。 機械が意図した目標を達成するためには、私たちを&quot;説得&quot;する必要があります。</p>
<!-- I would not fully accept my robot vacuum cleaner if it did not explain its behavior to some degree. -->
<p>ロボット掃除機がその動作をある程度説明できなければ、私は完全に受け入れることはできないでしょう。</p>
<!-- The vacuum cleaner creates a shared meaning of, for example, an "accident" (like getting stuck on the bathroom carpet ... again) by explaining that it got stuck instead of simply stopping to work without comment. -->
<p>掃除機は何も言わず単に停止するのではなく、その理由（例えばバスルームのカーペットに引っかかるといった「事故」など）を説明することで、共通の理解を生み出します。</p>
<!-- Interestingly, there may be a misalignment between the goal of the explaining machine (create trust) and the goal of the recipient (understand the prediction or behavior).
Perhaps the full explanation for why Doge got stuck could be that the battery was very low, that one of the wheels is not working properly and that there is a bug that makes the robot go to the same spot over and over again even though there was an obstacle. -->
<p>興味深いことに、説明する機械側の目的（信頼を築く）と受け手側の目的（予測、あるいは動作を理解する）の間に乖離が生じる可能性があります。 ひょっとしたら、Dogeが動かなくなった本当の理由は、バッテリーがとても少ないこと、タイヤの１つが故障していること、障害物があるにも関わらず同じ場所を何度も往復するバグがあることかもしれません。</p>
<!-- These reasons (and a few more) caused the robot to get stuck, but it only explained that something was in the way, and that was enough for me to trust its behavior and get a shared meaning of that accident.
By the way, Doge got stuck in the bathroom again.
We have to remove the carpets  each time before we let Doge vacuum. -->
<p>これらの理由（もしくは他のいくつかの理由）によってロボット掃除機は停止しましたが、私が彼の動きを信頼しその事故の共通の意味を理解するには、何かが邪魔をしていることを説明すれば十分でした。 ちなみに、Dogeはバスルームでまた立ち往生しました。 Dogeに掃除させる前に、毎回カーペットを退けておく必要があったみたいです。</p>
<!--fig.cap="Doge, our vacuum cleaner, got stuck. As an explanation for the accident, Doge told us that it needs to be on an even surface."-->
<div class="figure"><span id="fig:doge-stuck"></span>
<img src="images/doge-stuck.jpg" alt="掃除機 Doge が停止している。事故の説明として、Doge は平らな表面上に居なければならないと教えてくれた。" width="800" />
<p class="caption">
FIGURE 2.2: 掃除機 Doge が停止している。事故の説明として、Doge は平らな表面上に居なければならないと教えてくれた。
</p>
</div>
<!--
Machine learning models can only be **debugged and audited** when they can be interpreted.
Even in low risk environments, such as movie recommendations, the ability to interpret is valuable in the research and development phase as well as after deployment.
Later, when a model is used in a product, things can go wrong.
An interpretation for an erroneous prediction helps to understand the cause of the error.
It delivers a direction for how to fix the system.
Consider an example of a husky versus wolf classifier that misclassifies some huskies as wolves.
Using interpretable machine learning methods, you would find that the misclassification was due to the snow on the image.
The classifier learned to use snow as a feature for classifying images as "wolf", which might make sense in terms of separating wolves from huskies in the training dataset, but not in real-world use.
-->
<p>機械学習モデルは解釈可能な場合にのみ<strong>デバッグや検査</strong>ができます。 映画推薦のようなリスクが低い場合や、デプロイ後に限らず研究・調査段階であったとしても解釈可能性は価値があります。 モデルを製品の中で使用する場合、後になってから問題が発生することがあります。 誤った予測に対する解釈はエラーの原因を理解するために役立ちます。 これによって、システムをどのように修正するかという方向性が得られます。 ハスキーとオオカミの分類器において一部のハスキーをオオカミと誤分類する例を考えてみます。 解釈可能な機械学習手法を用いると、誤分類が画像上の雪によって生じていることがわかりました。 分類器は画像をオオカミと分類するための特徴として雪を学習しました。学習データにおいて、ハスキーとオオカミを分類するという観点からは意味があったかもしれませんが、実世界では、意味がありません。</p>
<!--
If you can ensure that the machine learning model can explain decisions, you can also check the following traits more easily (Doshi-Velez and Kim 2017):
- Fairness: Ensuring that predictions are unbiased and do not implicitly or explicitly discriminate against protected groups.
An interpretable model can tell you why it has decided that a certain person should not get a loan, and it becomes easier for a human to judge whether the decision is based on a learned demographic (e.g. racial) bias.
- Privacy: Ensuring that sensitive information in the data is protected.
- Reliability or Robustness: Ensuring that small changes in the input do not lead to large changes in the prediction.
- Causality: Check that only causal relationships are picked up. 
- Trust: It is easier for humans to trust a system that explains its decisions compared to a black box.
-->
<p>機械学習モデルが判断の根拠を説明できるようになれば、以下の特性も簡単に確かめることができます。 (Doshi-Velez and Kim 2017) - 公平性: 予測に偏りがなく、保護されるグループに対して明示的または暗黙的に差別しない。 解釈可能なモデルでは、ある人物が融資を受けるべきでないと決定した理由がわかり、その理由が学習した人口統計(人種など)の偏りに基づいているかどうかを人間が判断しやすくなります。 - プライバシー: データ内の機密情報が保護されていること。 - 信頼性または頑健性：入力の小さな変化が予測に大きな変化をもたらさないこと。 - 因果関係: 因果関係だけが取得されていること。 - 信頼: ブラックボックスよりも自身の決定を説明できるシステムの方が人間に信頼されやすいこと。</p>
<!--
**When we do not need interpretability.**
-->
<p><strong>解釈可能性を必要としない場合</strong></p>
<!--
The following scenarios illustrate when we do not need or even do not want interpretability of machine learning models.
-->
<p>以下のシナリオは機械学習モデルの解釈可能性がいつ必要とされないか、あるいはいつ求められないかを説明します。</p>
<!--
Interpretability is not required if the model **has no significant impact**.
Imagine someone named Mike working on a machine learning side project to predict where his friends will go for their next holidays based on Facebook data.
Mike just likes to surprise his friends with educated guesses where they will be going on holidays.
There is no real problem if the model is wrong (at worst just a little embarrassment for Mike), nor is there a problem if Mike cannot explain the output of his model.
It is perfectly fine not to have interpretability in this case.
The situation would change if Mike started building a business around these holiday destination predictions.
If the model is wrong, the business could lose money, or the model may work worse for some people because of learned racial bias.
As soon as the model has a significant impact, be it financial or social, interpretability becomes relevant.
-->
<p>モデルが<strong>重大な影響力を持たない</strong>ならば、解釈可能性は必要とされません。 フェイスブックのデータを元に友人が次の休暇にどこへ行くのかを予測する機械学習のプロジェクトで働くマイクという人物を想像してみてください。 マイクは友人が休暇中にどこへ行くのかを学習によって推測することで単に友人を驚かせるのが好きなのです。 もしモデルが間違えたとしても(最悪、マイクが少し恥ずかしい思いをするだけで)実害はありませんし、マイクがモデルの予測結果を説明できなくても問題はありません。 この場合では解釈可能性がなくても全く問題ないのです。 もしマイクが休暇中の行き先予測に関するビジネスを始めるならば、状況は変わります。 もしモデルが間違えれば、ビジネスでお金を失うか、あるいは人種に関する偏見を学習することでモデルが一部の人々にとって悪く働くかもしれません。 経済的であれ社会的であれ、モデルが重大な影響力を持つとすぐに、解釈可能性は重要になります。</p>
<!--
Interpretability is not required when the **problem is well studied**.
Some applications have been sufficiently well studied so that there is enough practical experience with the model and problems with the model have been solved over time.
A good example is a machine learning model for optical character recognition that processes images from envelopes and extracts addresses.
There is years of experience with these systems and it is clear that they work. 
In addition, we are not really interested in gaining additional insights about the task at hand. 
-->
<p><strong>問題が十分に研究されている</strong>ならば、解釈可能性は必要ありません。 一部のアプリケーションは十分に研究されているため、モデルに関する十分な実務経験があり、モデルの問題は時間をかけて解決されてきました。 その良い例は、封筒の画像を処理して住所を抽出する光学式文字認識の機械学習モデルです。 これらのシステムには長年の経験があり、それらが機能することは明らかです。 加えて、このタスクについて追加の洞察を得ることには関心がありません。</p>
<!--
Interpretability might enable people or programs to **manipulate the system**.
Problems with users who deceive a system result from a mismatch between the goals of the creator and the user of a model.
Credit scoring is such a system because banks want to ensure that loans are only given to applicants who are likely to return them, and applicants aim to get the loan even if the bank does not want to give them one.
This mismatch between the goals introduces incentives for applicants to game the system to increase their chances of getting a loan.
If an applicant knows that having more than two credit cards negatively affects his score, he simply returns his third credit card to improve his score, and organizes a new card after the loan has been approved.
While his score improved, the actual probability of repaying the loan remained unchanged.
The system can only be gamed if the inputs are proxies for a causal feature, but do not actually cause the outcome.
Whenever possible, proxy features should be avoided as they make models gameable.
For example, Google developed a system called Google Flu Trends to predict flu outbreaks.
The system correlated Google searches with flu outbreaks -- and it has performed poorly. 
The distribution of search queries changed and Google Flu Trends missed many flu outbreaks.
Google searches do not cause the flu.
When people search for symptoms like "fever" it is merely a correlation with actual flu outbreaks.
Ideally, models would only use causal features because they would not be gameable.
-->
<p>解釈可能性は人間やプログラムが<strong>システムを操作すること</strong>を可能にします。 システムを欺く問題はモデルの使用者と開発者間の目的の不一致によって発生します。 クレジットの信用スコアはそのようなシステムであり、銀行は返済能力のある申請者だけに融資されるようにしたい一方で、申請者はたとえ銀行が融資を望まない場合でも融資されることを目的とします。 この両者の目的の不一致は、申請者が融資を得る可能性を高めるためにシステムを操る動機となります。 ２枚より多くのクレジットカードを所持していると数値に悪影響があると申請者が知っていれば、数値を向上させるために３枚目のカードを一旦返却し、融資が成立した後に新たなカードを発行するでしょう。 これは数値が改善される一方で、融資を返済する実際の可能性は変わりません。 このシステムは、入力が因果的特徴の代理となっている場合であって、出力との実際の因果関係とは異なる場合に操られるでしょう。 モデルをゲーム化させないためにも、代理的な特徴は避けるべきです。 例えば、グーグルはインフルエンザの流行を予測するためにGoogle Flu Trendsと呼ばれるシステムを開発しました。 そのシステムはGoogle検索とインフルエンザの流行を関連付けましたが、機能は低下してしまいました。 検索クエリの分布が変化したことで、Google Flu Trendsは多くのインフルエンザの流行を見逃してしまいました。 グーグル検索はインフルエンザを引き起こす原因ではないのです。 人々が”発熱”のような症状を検索する時と、実際のインフルエンザの流行とは単なる相関関係にすぎません。 理想的なのは、ゲーム化しないようにモデルが因果的な特徴だけを使用することです。</p>
<!--{pagebreak}-->
<!--
## Taxonomy of Interpretability Methods
-->
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p>Doshi-Velez, Finale, and Been Kim. &quot;Towards a rigorous science of interpretable machine learning,&quot; no. Ml: 1–13. <a href="http://arxiv.org/abs/1702.08608" class="uri">http://arxiv.org/abs/1702.08608</a> ( 2017).<a href="interpretability-importance.html#fnref5">↩</a></p></li>
<li id="fn6"><p>Doshi-Velez, Finale, and Been Kim. &quot;Towards a rigorous science of interpretable machine learning,&quot; no. Ml: 1–13. <a href="http://arxiv.org/abs/1702.08608" class="uri">http://arxiv.org/abs/1702.08608</a> ( 2017).<a href="interpretability-importance.html#fnref6">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="interpretability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="解釈可能な手法の分類.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/02-interpretability.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
