<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.6 人間に優しい説明 | Interpretable Machine Learning</title>
  <meta name="description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2.6 人間に優しい説明 | Interpretable Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  <meta name="github-repo" content="christophM/interpretable-ml-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.6 人間に優しい説明 | Interpretable Machine Learning" />
  
  <meta name="twitter:description" content="Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2020-12-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="properties.html"/>
<link rel="next" href="data.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<!-- Global site tag (gtag.js) - Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
  'anonymizeIp': true
  , 'storage': 'none'
  , 'clientId': window.localStorage.getItem('ga_clientId')
});
ga(function(tracker) {
  window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
});
ga('send', 'pageview');
</script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpretable machine learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>要約</a></li>
<li class="chapter" data-level="" data-path="著者による序文.html"><a href="著者による序文.html"><i class="fa fa-check"></i>著者による序文</a></li>
<li class="chapter" data-level="" data-path="preface-by-the-translator.html"><a href="preface-by-the-translator.html"><i class="fa fa-check"></i>Preface by the Translator</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> イントロダクション</a><ul>
<li class="chapter" data-level="1.1" data-path="storytime.html"><a href="storytime.html"><i class="fa fa-check"></i><b>1.1</b> 物語の時間</a><ul>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#信用失墜"><i class="fa fa-check"></i>信用失墜</a></li>
<li class="chapter" data-level="" data-path="storytime.html"><a href="storytime.html#フェルミのペーパークリップ"><i class="fa fa-check"></i>フェルミのペーパー・クリップ</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="機械学習とは何か.html"><a href="機械学習とは何か.html"><i class="fa fa-check"></i><b>1.2</b> 機械学習とは何か？</a></li>
<li class="chapter" data-level="1.3" data-path="terminology.html"><a href="terminology.html"><i class="fa fa-check"></i><b>1.3</b> 専門用語</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="interpretability.html"><a href="interpretability.html"><i class="fa fa-check"></i><b>2</b> 解釈可能性</a><ul>
<li class="chapter" data-level="2.1" data-path="interpretability-importance.html"><a href="interpretability-importance.html"><i class="fa fa-check"></i><b>2.1</b> 解釈可能性の重要性</a></li>
<li class="chapter" data-level="2.2" data-path="解釈可能な手法の分類.html"><a href="解釈可能な手法の分類.html"><i class="fa fa-check"></i><b>2.2</b> 解釈可能な手法の分類</a></li>
<li class="chapter" data-level="2.3" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html"><i class="fa fa-check"></i><b>2.3</b> 解釈可能性の範囲</a><ul>
<li class="chapter" data-level="2.3.1" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#全体的なモデルの解釈可能性"><i class="fa fa-check"></i><b>2.3.1</b> 全体的なモデルの解釈可能性</a></li>
<li class="chapter" data-level="2.3.2" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#モジュールレベルのモデルの全体的な解釈可能性"><i class="fa fa-check"></i><b>2.3.2</b> モジュールレベルのモデルの全体的な解釈可能性</a></li>
<li class="chapter" data-level="2.3.3" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#単一の予測に対する局所的な解釈"><i class="fa fa-check"></i><b>2.3.3</b> 単一の予測に対する局所的な解釈</a></li>
<li class="chapter" data-level="2.3.4" data-path="解釈可能性の範囲.html"><a href="解釈可能性の範囲.html#予測のグループに対する局所的な解釈"><i class="fa fa-check"></i><b>2.3.4</b> 予測のグループに対する局所的な解釈</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="解釈可能性の評価.html"><a href="解釈可能性の評価.html"><i class="fa fa-check"></i><b>2.4</b> 解釈可能性の評価</a></li>
<li class="chapter" data-level="2.5" data-path="properties.html"><a href="properties.html"><i class="fa fa-check"></i><b>2.5</b> 説明に関する性質</a></li>
<li class="chapter" data-level="2.6" data-path="explanation.html"><a href="explanation.html"><i class="fa fa-check"></i><b>2.6</b> 人間に優しい説明</a><ul>
<li class="chapter" data-level="2.6.1" data-path="explanation.html"><a href="explanation.html#説明とはなにか"><i class="fa fa-check"></i><b>2.6.1</b> 説明とはなにか</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>3</b> データセット</a><ul>
<li class="chapter" data-level="3.1" data-path="bike-data.html"><a href="bike-data.html"><i class="fa fa-check"></i><b>3.1</b> 自転車レンタル (回帰)</a></li>
<li class="chapter" data-level="3.2" data-path="spam-data.html"><a href="spam-data.html"><i class="fa fa-check"></i><b>3.2</b> YouTube スパムコメント (テキスト分類)</a></li>
<li class="chapter" data-level="3.3" data-path="cervical.html"><a href="cervical.html"><i class="fa fa-check"></i><b>3.3</b> 子宮頸がんのリスク要因(クラス分類)</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>4</b> Interpretable Models</a><ul>
<li class="chapter" data-level="4.1" data-path="limo.html"><a href="limo.html"><i class="fa fa-check"></i><b>4.1</b> Linear Regression</a><ul>
<li class="chapter" data-level="4.1.1" data-path="limo.html"><a href="limo.html#interpretation"><i class="fa fa-check"></i><b>4.1.1</b> Interpretation</a></li>
<li class="chapter" data-level="4.1.2" data-path="limo.html"><a href="limo.html#example"><i class="fa fa-check"></i><b>4.1.2</b> Example</a></li>
<li class="chapter" data-level="4.1.3" data-path="limo.html"><a href="limo.html#visual-interpretation"><i class="fa fa-check"></i><b>4.1.3</b> Visual Interpretation</a></li>
<li class="chapter" data-level="4.1.4" data-path="limo.html"><a href="limo.html#explain-individual-predictions"><i class="fa fa-check"></i><b>4.1.4</b> Explain Individual Predictions</a></li>
<li class="chapter" data-level="4.1.5" data-path="limo.html"><a href="limo.html#cat-code"><i class="fa fa-check"></i><b>4.1.5</b> Encoding of Categorical Features</a></li>
<li class="chapter" data-level="4.1.6" data-path="limo.html"><a href="limo.html#do-linear-models-create-good-explanations"><i class="fa fa-check"></i><b>4.1.6</b> Do Linear Models Create Good Explanations?</a></li>
<li class="chapter" data-level="4.1.7" data-path="limo.html"><a href="limo.html#sparse-linear"><i class="fa fa-check"></i><b>4.1.7</b> Sparse Linear Models</a></li>
<li class="chapter" data-level="4.1.8" data-path="limo.html"><a href="limo.html#advantages"><i class="fa fa-check"></i><b>4.1.8</b> Advantages</a></li>
<li class="chapter" data-level="4.1.9" data-path="limo.html"><a href="limo.html#disadvantages"><i class="fa fa-check"></i><b>4.1.9</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic.html"><a href="logistic.html"><i class="fa fa-check"></i><b>4.2</b> ロジスティック回帰</a><ul>
<li class="chapter" data-level="4.2.1" data-path="logistic.html"><a href="logistic.html#理論"><i class="fa fa-check"></i><b>4.2.1</b> 理論</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="解釈性.html"><a href="解釈性.html"><i class="fa fa-check"></i><b>4.3</b> 解釈性</a><ul>
<li class="chapter" data-level="4.3.1" data-path="解釈性.html"><a href="解釈性.html#例"><i class="fa fa-check"></i><b>4.3.1</b> 例</a></li>
<li class="chapter" data-level="4.3.2" data-path="解釈性.html"><a href="解釈性.html#長所と短所"><i class="fa fa-check"></i><b>4.3.2</b> 長所と短所</a></li>
<li class="chapter" data-level="4.3.3" data-path="解釈性.html"><a href="解釈性.html#ソフトウェア"><i class="fa fa-check"></i><b>4.3.3</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="extend-lm.html"><a href="extend-lm.html"><i class="fa fa-check"></i><b>4.4</b> GLM, GAM and more</a><ul>
<li class="chapter" data-level="4.4.1" data-path="extend-lm.html"><a href="extend-lm.html#glm"><i class="fa fa-check"></i><b>4.4.1</b> Non-Gaussian Outcomes - GLMs</a></li>
<li class="chapter" data-level="4.4.2" data-path="extend-lm.html"><a href="extend-lm.html#lm-interact"><i class="fa fa-check"></i><b>4.4.2</b> Interactions</a></li>
<li class="chapter" data-level="4.4.3" data-path="extend-lm.html"><a href="extend-lm.html#gam"><i class="fa fa-check"></i><b>4.4.3</b> Nonlinear Effects - GAMs</a></li>
<li class="chapter" data-level="4.4.4" data-path="extend-lm.html"><a href="extend-lm.html#advantages-1"><i class="fa fa-check"></i><b>4.4.4</b> Advantages</a></li>
<li class="chapter" data-level="4.4.5" data-path="extend-lm.html"><a href="extend-lm.html#disadvantages-1"><i class="fa fa-check"></i><b>4.4.5</b> Disadvantages</a></li>
<li class="chapter" data-level="4.4.6" data-path="extend-lm.html"><a href="extend-lm.html#software"><i class="fa fa-check"></i><b>4.4.6</b> Software</a></li>
<li class="chapter" data-level="4.4.7" data-path="extend-lm.html"><a href="extend-lm.html#more-lm-extension"><i class="fa fa-check"></i><b>4.4.7</b> Further Extensions</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="tree.html"><a href="tree.html"><i class="fa fa-check"></i><b>4.5</b> 決定木</a><ul>
<li class="chapter" data-level="4.5.1" data-path="tree.html"><a href="tree.html#決定木の解釈"><i class="fa fa-check"></i><b>4.5.1</b> 決定木の解釈</a></li>
<li class="chapter" data-level="4.5.2" data-path="tree.html"><a href="tree.html#例-1"><i class="fa fa-check"></i><b>4.5.2</b> 例</a></li>
<li class="chapter" data-level="4.5.3" data-path="tree.html"><a href="tree.html#長所"><i class="fa fa-check"></i><b>4.5.3</b> 長所</a></li>
<li class="chapter" data-level="4.5.4" data-path="tree.html"><a href="tree.html#短所"><i class="fa fa-check"></i><b>4.5.4</b> 短所</a></li>
<li class="chapter" data-level="4.5.5" data-path="tree.html"><a href="tree.html#ソフトウェア-1"><i class="fa fa-check"></i><b>4.5.5</b> ソフトウェア</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rules.html"><a href="rules.html"><i class="fa fa-check"></i><b>4.6</b> Decision Rules</a><ul>
<li class="chapter" data-level="4.6.1" data-path="rules.html"><a href="rules.html#learn-rules-from-a-single-feature-oner"><i class="fa fa-check"></i><b>4.6.1</b> Learn Rules from a Single Feature (OneR)</a></li>
<li class="chapter" data-level="4.6.2" data-path="rules.html"><a href="rules.html#sequential-covering"><i class="fa fa-check"></i><b>4.6.2</b> Sequential Covering</a></li>
<li class="chapter" data-level="4.6.3" data-path="rules.html"><a href="rules.html#bayesian-rule-lists"><i class="fa fa-check"></i><b>4.6.3</b> Bayesian Rule Lists</a></li>
<li class="chapter" data-level="4.6.4" data-path="rules.html"><a href="rules.html#advantages-2"><i class="fa fa-check"></i><b>4.6.4</b> Advantages</a></li>
<li class="chapter" data-level="4.6.5" data-path="rules.html"><a href="rules.html#disadvantages-2"><i class="fa fa-check"></i><b>4.6.5</b> Disadvantages</a></li>
<li class="chapter" data-level="4.6.6" data-path="rules.html"><a href="rules.html#software-and-alternatives"><i class="fa fa-check"></i><b>4.6.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="rulefit.html"><a href="rulefit.html"><i class="fa fa-check"></i><b>4.7</b> RuleFit</a><ul>
<li class="chapter" data-level="4.7.1" data-path="rulefit.html"><a href="rulefit.html#interpretation-and-example"><i class="fa fa-check"></i><b>4.7.1</b> Interpretation and Example</a></li>
<li class="chapter" data-level="4.7.2" data-path="rulefit.html"><a href="rulefit.html#theory"><i class="fa fa-check"></i><b>4.7.2</b> Theory</a></li>
<li class="chapter" data-level="4.7.3" data-path="rulefit.html"><a href="rulefit.html#advantages-3"><i class="fa fa-check"></i><b>4.7.3</b> Advantages</a></li>
<li class="chapter" data-level="4.7.4" data-path="rulefit.html"><a href="rulefit.html#disadvantages-3"><i class="fa fa-check"></i><b>4.7.4</b> Disadvantages</a></li>
<li class="chapter" data-level="4.7.5" data-path="rulefit.html"><a href="rulefit.html#software-and-alternative"><i class="fa fa-check"></i><b>4.7.5</b> Software and Alternative</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="other-interpretable.html"><a href="other-interpretable.html"><i class="fa fa-check"></i><b>4.8</b> Other Interpretable Models</a><ul>
<li class="chapter" data-level="4.8.1" data-path="other-interpretable.html"><a href="other-interpretable.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>4.8.1</b> Naive Bayes Classifier</a></li>
<li class="chapter" data-level="4.8.2" data-path="other-interpretable.html"><a href="other-interpretable.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>4.8.2</b> K-Nearest Neighbors</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="agnostic.html"><a href="agnostic.html"><i class="fa fa-check"></i><b>5</b> Model-Agnostic Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="pdp.html"><a href="pdp.html"><i class="fa fa-check"></i><b>5.1</b> Partial Dependence Plot (PDP)</a><ul>
<li class="chapter" data-level="5.1.1" data-path="pdp.html"><a href="pdp.html#例-2"><i class="fa fa-check"></i><b>5.1.1</b> 例</a></li>
<li class="chapter" data-level="5.1.2" data-path="pdp.html"><a href="pdp.html#長所-1"><i class="fa fa-check"></i><b>5.1.2</b> 長所</a></li>
<li class="chapter" data-level="5.1.3" data-path="pdp.html"><a href="pdp.html#短所-1"><i class="fa fa-check"></i><b>5.1.3</b> 短所</a></li>
<li class="chapter" data-level="5.1.4" data-path="pdp.html"><a href="pdp.html#ソフトウェアと代替手法"><i class="fa fa-check"></i><b>5.1.4</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="ice.html"><a href="ice.html"><i class="fa fa-check"></i><b>5.2</b> Individual Conditional Expectation (ICE)</a><ul>
<li class="chapter" data-level="5.2.1" data-path="ice.html"><a href="ice.html#例-3"><i class="fa fa-check"></i><b>5.2.1</b> 例</a></li>
<li class="chapter" data-level="5.2.2" data-path="ice.html"><a href="ice.html#長所-2"><i class="fa fa-check"></i><b>5.2.2</b> 長所</a></li>
<li class="chapter" data-level="5.2.3" data-path="ice.html"><a href="ice.html#短所-2"><i class="fa fa-check"></i><b>5.2.3</b> 短所</a></li>
<li class="chapter" data-level="5.2.4" data-path="ice.html"><a href="ice.html#ソフトウェアと代替手法-1"><i class="fa fa-check"></i><b>5.2.4</b> ソフトウェアと代替手法</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="ale.html"><a href="ale.html"><i class="fa fa-check"></i><b>5.3</b> Accumulated Local Effects (ALE) Plot</a><ul>
<li class="chapter" data-level="5.3.1" data-path="ale.html"><a href="ale.html#motivation-and-intuition"><i class="fa fa-check"></i><b>5.3.1</b> Motivation and Intuition</a></li>
<li class="chapter" data-level="5.3.2" data-path="ale.html"><a href="ale.html#theory-1"><i class="fa fa-check"></i><b>5.3.2</b> Theory</a></li>
<li class="chapter" data-level="5.3.3" data-path="ale.html"><a href="ale.html#estimation"><i class="fa fa-check"></i><b>5.3.3</b> Estimation</a></li>
<li class="chapter" data-level="5.3.4" data-path="ale.html"><a href="ale.html#examples"><i class="fa fa-check"></i><b>5.3.4</b> Examples</a></li>
<li class="chapter" data-level="5.3.5" data-path="ale.html"><a href="ale.html#advantages-4"><i class="fa fa-check"></i><b>5.3.5</b> Advantages</a></li>
<li class="chapter" data-level="5.3.6" data-path="ale.html"><a href="ale.html#disadvantages-4"><i class="fa fa-check"></i><b>5.3.6</b> Disadvantages</a></li>
<li class="chapter" data-level="5.3.7" data-path="ale.html"><a href="ale.html#implementation-and-alternatives"><i class="fa fa-check"></i><b>5.3.7</b> Implementation and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="interaction.html"><a href="interaction.html"><i class="fa fa-check"></i><b>5.4</b> Feature Interaction</a><ul>
<li class="chapter" data-level="5.4.1" data-path="interaction.html"><a href="interaction.html#feature-interaction"><i class="fa fa-check"></i><b>5.4.1</b> Feature Interaction?</a></li>
<li class="chapter" data-level="5.4.2" data-path="interaction.html"><a href="interaction.html#theory-friedmans-h-statistic"><i class="fa fa-check"></i><b>5.4.2</b> Theory: Friedman's H-statistic</a></li>
<li class="chapter" data-level="5.4.3" data-path="interaction.html"><a href="interaction.html#examples-1"><i class="fa fa-check"></i><b>5.4.3</b> Examples</a></li>
<li class="chapter" data-level="5.4.4" data-path="interaction.html"><a href="interaction.html#advantages-5"><i class="fa fa-check"></i><b>5.4.4</b> Advantages</a></li>
<li class="chapter" data-level="5.4.5" data-path="interaction.html"><a href="interaction.html#disadvantages-5"><i class="fa fa-check"></i><b>5.4.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.4.6" data-path="interaction.html"><a href="interaction.html#implementations"><i class="fa fa-check"></i><b>5.4.6</b> Implementations</a></li>
<li class="chapter" data-level="5.4.7" data-path="interaction.html"><a href="interaction.html#alternatives"><i class="fa fa-check"></i><b>5.4.7</b> Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="feature-importance.html"><a href="feature-importance.html"><i class="fa fa-check"></i><b>5.5</b> Permutation Feature Importance</a><ul>
<li class="chapter" data-level="5.5.1" data-path="feature-importance.html"><a href="feature-importance.html#theory-2"><i class="fa fa-check"></i><b>5.5.1</b> Theory</a></li>
<li class="chapter" data-level="5.5.2" data-path="feature-importance.html"><a href="feature-importance.html#feature-importance-data"><i class="fa fa-check"></i><b>5.5.2</b> Should I Compute Importance on Training or Test Data?</a></li>
<li class="chapter" data-level="5.5.3" data-path="feature-importance.html"><a href="feature-importance.html#example-and-interpretation"><i class="fa fa-check"></i><b>5.5.3</b> Example and Interpretation</a></li>
<li class="chapter" data-level="5.5.4" data-path="feature-importance.html"><a href="feature-importance.html#advantages-6"><i class="fa fa-check"></i><b>5.5.4</b> Advantages</a></li>
<li class="chapter" data-level="5.5.5" data-path="feature-importance.html"><a href="feature-importance.html#disadvantages-6"><i class="fa fa-check"></i><b>5.5.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.5.6" data-path="feature-importance.html"><a href="feature-importance.html#software-and-alternatives-1"><i class="fa fa-check"></i><b>5.5.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="global.html"><a href="global.html"><i class="fa fa-check"></i><b>5.6</b> Global Surrogate</a><ul>
<li class="chapter" data-level="5.6.1" data-path="global.html"><a href="global.html#theory-3"><i class="fa fa-check"></i><b>5.6.1</b> Theory</a></li>
<li class="chapter" data-level="5.6.2" data-path="global.html"><a href="global.html#example-1"><i class="fa fa-check"></i><b>5.6.2</b> Example</a></li>
<li class="chapter" data-level="5.6.3" data-path="global.html"><a href="global.html#advantages-7"><i class="fa fa-check"></i><b>5.6.3</b> Advantages</a></li>
<li class="chapter" data-level="5.6.4" data-path="global.html"><a href="global.html#disadvantages-7"><i class="fa fa-check"></i><b>5.6.4</b> Disadvantages</a></li>
<li class="chapter" data-level="5.6.5" data-path="global.html"><a href="global.html#software-1"><i class="fa fa-check"></i><b>5.6.5</b> Software</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="lime.html"><a href="lime.html"><i class="fa fa-check"></i><b>5.7</b> Local Surrogate (LIME)</a><ul>
<li class="chapter" data-level="5.7.1" data-path="lime.html"><a href="lime.html#lime-for-tabular-data"><i class="fa fa-check"></i><b>5.7.1</b> LIME for Tabular Data</a></li>
<li class="chapter" data-level="5.7.2" data-path="lime.html"><a href="lime.html#lime-for-text"><i class="fa fa-check"></i><b>5.7.2</b> LIME for Text</a></li>
<li class="chapter" data-level="5.7.3" data-path="lime.html"><a href="lime.html#images-lime"><i class="fa fa-check"></i><b>5.7.3</b> LIME for Images</a></li>
<li class="chapter" data-level="5.7.4" data-path="lime.html"><a href="lime.html#advantages-8"><i class="fa fa-check"></i><b>5.7.4</b> Advantages</a></li>
<li class="chapter" data-level="5.7.5" data-path="lime.html"><a href="lime.html#disadvantages-8"><i class="fa fa-check"></i><b>5.7.5</b> Disadvantages</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="anchors.html"><a href="anchors.html"><i class="fa fa-check"></i><b>5.8</b> Scoped Rules (Anchors)</a><ul>
<li class="chapter" data-level="5.8.1" data-path="anchors.html"><a href="anchors.html#finding-anchors"><i class="fa fa-check"></i><b>5.8.1</b> Finding Anchors</a></li>
<li class="chapter" data-level="5.8.2" data-path="anchors.html"><a href="anchors.html#complexity-and-runtime"><i class="fa fa-check"></i><b>5.8.2</b> Complexity and Runtime</a></li>
<li class="chapter" data-level="5.8.3" data-path="anchors.html"><a href="anchors.html#tabular-data-example"><i class="fa fa-check"></i><b>5.8.3</b> Tabular Data Example</a></li>
<li class="chapter" data-level="5.8.4" data-path="anchors.html"><a href="anchors.html#advantages-9"><i class="fa fa-check"></i><b>5.8.4</b> Advantages</a></li>
<li class="chapter" data-level="5.8.5" data-path="anchors.html"><a href="anchors.html#disadvantages-9"><i class="fa fa-check"></i><b>5.8.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.8.6" data-path="anchors.html"><a href="anchors.html#software-and-alternatives-2"><i class="fa fa-check"></i><b>5.8.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="shapley.html"><a href="shapley.html"><i class="fa fa-check"></i><b>5.9</b> Shapley Values</a><ul>
<li class="chapter" data-level="5.9.1" data-path="shapley.html"><a href="shapley.html#general-idea"><i class="fa fa-check"></i><b>5.9.1</b> General Idea</a></li>
<li class="chapter" data-level="5.9.2" data-path="shapley.html"><a href="shapley.html#examples-and-interpretation"><i class="fa fa-check"></i><b>5.9.2</b> Examples and Interpretation</a></li>
<li class="chapter" data-level="5.9.3" data-path="shapley.html"><a href="shapley.html#the-shapley-value-in-detail"><i class="fa fa-check"></i><b>5.9.3</b> The Shapley Value in Detail</a></li>
<li class="chapter" data-level="5.9.4" data-path="shapley.html"><a href="shapley.html#advantages-10"><i class="fa fa-check"></i><b>5.9.4</b> Advantages</a></li>
<li class="chapter" data-level="5.9.5" data-path="shapley.html"><a href="shapley.html#disadvantages-10"><i class="fa fa-check"></i><b>5.9.5</b> Disadvantages</a></li>
<li class="chapter" data-level="5.9.6" data-path="shapley.html"><a href="shapley.html#software-and-alternatives-3"><i class="fa fa-check"></i><b>5.9.6</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="shap.html"><a href="shap.html"><i class="fa fa-check"></i><b>5.10</b> SHAP (SHapley Additive exPlanations)</a><ul>
<li class="chapter" data-level="5.10.1" data-path="shap.html"><a href="shap.html#definition"><i class="fa fa-check"></i><b>5.10.1</b> Definition</a></li>
<li class="chapter" data-level="5.10.2" data-path="shap.html"><a href="shap.html#kernelshap"><i class="fa fa-check"></i><b>5.10.2</b> KernelSHAP</a></li>
<li class="chapter" data-level="5.10.3" data-path="shap.html"><a href="shap.html#treeshap"><i class="fa fa-check"></i><b>5.10.3</b> TreeSHAP</a></li>
<li class="chapter" data-level="5.10.4" data-path="shap.html"><a href="shap.html#examples-2"><i class="fa fa-check"></i><b>5.10.4</b> Examples</a></li>
<li class="chapter" data-level="5.10.5" data-path="shap.html"><a href="shap.html#shap-feature-importance"><i class="fa fa-check"></i><b>5.10.5</b> SHAP Feature Importance</a></li>
<li class="chapter" data-level="5.10.6" data-path="shap.html"><a href="shap.html#shap-summary-plot"><i class="fa fa-check"></i><b>5.10.6</b> SHAP Summary Plot</a></li>
<li class="chapter" data-level="5.10.7" data-path="shap.html"><a href="shap.html#shap-dependence-plot"><i class="fa fa-check"></i><b>5.10.7</b> SHAP Dependence Plot</a></li>
<li class="chapter" data-level="5.10.8" data-path="shap.html"><a href="shap.html#shap-interaction-values"><i class="fa fa-check"></i><b>5.10.8</b> SHAP Interaction Values</a></li>
<li class="chapter" data-level="5.10.9" data-path="shap.html"><a href="shap.html#clustering-shap-values"><i class="fa fa-check"></i><b>5.10.9</b> Clustering SHAP values</a></li>
<li class="chapter" data-level="5.10.10" data-path="shap.html"><a href="shap.html#advantages-11"><i class="fa fa-check"></i><b>5.10.10</b> Advantages</a></li>
<li class="chapter" data-level="5.10.11" data-path="shap.html"><a href="shap.html#disadvantages-11"><i class="fa fa-check"></i><b>5.10.11</b> Disadvantages</a></li>
<li class="chapter" data-level="5.10.12" data-path="shap.html"><a href="shap.html#software-2"><i class="fa fa-check"></i><b>5.10.12</b> Software</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="example-based.html"><a href="example-based.html"><i class="fa fa-check"></i><b>6</b> Example-Based Explanations</a><ul>
<li class="chapter" data-level="6.1" data-path="counterfactual.html"><a href="counterfactual.html"><i class="fa fa-check"></i><b>6.1</b> Counterfactual Explanations</a><ul>
<li class="chapter" data-level="6.1.1" data-path="counterfactual.html"><a href="counterfactual.html#generating-counterfactual-explanations"><i class="fa fa-check"></i><b>6.1.1</b> Generating Counterfactual Explanations</a></li>
<li class="chapter" data-level="6.1.2" data-path="counterfactual.html"><a href="counterfactual.html#examples-3"><i class="fa fa-check"></i><b>6.1.2</b> Examples</a></li>
<li class="chapter" data-level="6.1.3" data-path="counterfactual.html"><a href="counterfactual.html#advantages-12"><i class="fa fa-check"></i><b>6.1.3</b> Advantages</a></li>
<li class="chapter" data-level="6.1.4" data-path="counterfactual.html"><a href="counterfactual.html#disadvantages-12"><i class="fa fa-check"></i><b>6.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="6.1.5" data-path="counterfactual.html"><a href="counterfactual.html#example-software"><i class="fa fa-check"></i><b>6.1.5</b> Software and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="adversarial.html"><a href="adversarial.html"><i class="fa fa-check"></i><b>6.2</b> Adversarial Examples</a><ul>
<li class="chapter" data-level="6.2.1" data-path="adversarial.html"><a href="adversarial.html#methods-and-examples"><i class="fa fa-check"></i><b>6.2.1</b> Methods and Examples</a></li>
<li class="chapter" data-level="6.2.2" data-path="adversarial.html"><a href="adversarial.html#the-cybersecurity-perspective"><i class="fa fa-check"></i><b>6.2.2</b> The Cybersecurity Perspective</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="proto.html"><a href="proto.html"><i class="fa fa-check"></i><b>6.3</b> Prototypes and Criticisms</a><ul>
<li class="chapter" data-level="6.3.1" data-path="proto.html"><a href="proto.html#theory-4"><i class="fa fa-check"></i><b>6.3.1</b> Theory</a></li>
<li class="chapter" data-level="6.3.2" data-path="proto.html"><a href="proto.html#examples-4"><i class="fa fa-check"></i><b>6.3.2</b> Examples</a></li>
<li class="chapter" data-level="6.3.3" data-path="proto.html"><a href="proto.html#advantages-13"><i class="fa fa-check"></i><b>6.3.3</b> Advantages</a></li>
<li class="chapter" data-level="6.3.4" data-path="proto.html"><a href="proto.html#disadvantages-13"><i class="fa fa-check"></i><b>6.3.4</b> Disadvantages</a></li>
<li class="chapter" data-level="6.3.5" data-path="proto.html"><a href="proto.html#code-and-alternatives"><i class="fa fa-check"></i><b>6.3.5</b> Code and Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="influential.html"><a href="influential.html"><i class="fa fa-check"></i><b>6.4</b> Influential Instances</a><ul>
<li class="chapter" data-level="6.4.1" data-path="influential.html"><a href="influential.html#deletion-diagnostics"><i class="fa fa-check"></i><b>6.4.1</b> Deletion Diagnostics</a></li>
<li class="chapter" data-level="6.4.2" data-path="influential.html"><a href="influential.html#influence-functions"><i class="fa fa-check"></i><b>6.4.2</b> Influence Functions</a></li>
<li class="chapter" data-level="6.4.3" data-path="influential.html"><a href="influential.html#advantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>6.4.3</b> Advantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="6.4.4" data-path="influential.html"><a href="influential.html#disadvantages-of-identifying-influential-instances"><i class="fa fa-check"></i><b>6.4.4</b> Disadvantages of Identifying Influential Instances</a></li>
<li class="chapter" data-level="6.4.5" data-path="influential.html"><a href="influential.html#software-and-alternatives-4"><i class="fa fa-check"></i><b>6.4.5</b> Software and Alternatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>7</b> Neural Network Interpretation</a><ul>
<li class="chapter" data-level="7.1" data-path="cnn-features.html"><a href="cnn-features.html"><i class="fa fa-check"></i><b>7.1</b> Learned Features</a><ul>
<li class="chapter" data-level="7.1.1" data-path="cnn-features.html"><a href="cnn-features.html#feature-visualization"><i class="fa fa-check"></i><b>7.1.1</b> Feature Visualization</a></li>
<li class="chapter" data-level="7.1.2" data-path="cnn-features.html"><a href="cnn-features.html#network-dissection"><i class="fa fa-check"></i><b>7.1.2</b> Network Dissection</a></li>
<li class="chapter" data-level="7.1.3" data-path="cnn-features.html"><a href="cnn-features.html#advantages-14"><i class="fa fa-check"></i><b>7.1.3</b> Advantages</a></li>
<li class="chapter" data-level="7.1.4" data-path="cnn-features.html"><a href="cnn-features.html#disadvantages-14"><i class="fa fa-check"></i><b>7.1.4</b> Disadvantages</a></li>
<li class="chapter" data-level="7.1.5" data-path="cnn-features.html"><a href="cnn-features.html#software-and-further-material"><i class="fa fa-check"></i><b>7.1.5</b> Software and Further Material</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>8</b> A Look into the Crystal Ball</a><ul>
<li class="chapter" data-level="8.1" data-path="the-future-of-machine-learning.html"><a href="the-future-of-machine-learning.html"><i class="fa fa-check"></i><b>8.1</b> The Future of Machine Learning</a></li>
<li class="chapter" data-level="8.2" data-path="the-future-of-interpretability.html"><a href="the-future-of-interpretability.html"><i class="fa fa-check"></i><b>8.2</b> The Future of Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="contribute.html"><a href="contribute.html"><i class="fa fa-check"></i><b>9</b> Contribute to the Book</a></li>
<li class="chapter" data-level="10" data-path="cite.html"><a href="cite.html"><i class="fa fa-check"></i><b>10</b> Citing this Book</a></li>
<li class="chapter" data-level="11" data-path="translations.html"><a href="translations.html"><i class="fa fa-check"></i><b>11</b> Translations</a></li>
<li class="chapter" data-level="12" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>12</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a><ul>
<li class="chapter" data-level="" data-path="r-packages-used-for-examples.html"><a href="r-packages-used-for-examples.html"><i class="fa fa-check"></i>R Packages Used for Examples</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpretable Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="explanation" class="section level2">
<h2><span class="header-section-number">2.6</span> 人間に優しい説明</h2>
<!-- Let us dig deeper and discover what we humans see as "good" explanations and what the implications are for interpretable machine learning.
Humanities research can help us find out. -->
<p>私たち人間にとっての「良い」説明についてや、解釈可能な機械学習との密接な関係についてさらに深く掘り下げてみましょう。 それには人文科学が参考になるでしょう。</p>
<!-- Miller (2017) has conducted a huge survey of publications on explanations, and this chapter builds on his summary. -->
<p>Miller(2017)は、{explanations}についての出版物に対して、膨大な調査をしました。この章はその要約に基づいています。</p>
<!-- In this chapter, I want to convince you of the following:
As an explanation for an event, humans prefer short explanations (only 1 or 2 causes) that contrast the current situation with a situation in which the event would not have occurred. -->
<p>この章では、以下のことを説明します。 ある出来事の説明として、人間はその出来事が起こらなかったであろう状況と対比するような、1つか2つの要因でできた端的な説明を好みます。</p>
<!-- Especially abnormal causes provide good explanations.
Explanations are social interactions between the explainer and the explainee (recipient of the explanation) and therefore the social context has a great influence on the actual content of the explanation. -->
<p>特に、異常の原因は良い説明となります。 説明は、説明する人と説明を受ける人の間の社会的な相互作用なので、社会的な文脈は実際の説明の内容に大きな影響を与えます。</p>
<!-- When you need explanations with ALL factors for a particular prediction or behavior, you do not want a human-friendly explanation, but a complete causal attribution. -->
<p>特定の予測や行動に対して全ての要因を用いた説明をする必要がある場合は、人間に分かりやすい説明が求められている時ではなく、完全な因果関係が求められる場合です。</p>
<!-- You probably want a causal attribution if you are legally required to specify all influencing features or if you debug the machine learning model.
In this case, ignore the following points.
In all other cases, where lay people or people with little time are the recipients of the explanation, the following sections should be interesting to you. -->
<p>例えば、機械学習モデルをデバッグする場合や、特徴量の全ての影響を特定することが法的に求められている場合、おそらく因果属性が必要になるでしょう。このような場合は、以下のことは無視してください。そうでない場合、つまり一般の人々や時間のない人々へ説明する場合、次のセクションはきっと興味深いものとなります。</p>
<!-- ### What Is an Explanation? -->
<div id="説明とはなにか" class="section level3">
<h3><span class="header-section-number">2.6.1</span> 説明とはなにか</h3>
<!-- An explanation is the **answer to a why-question** (Miller 2017). -->
<p>説明とは<strong>原因を問う疑問への解答</strong>です。(Miller 2017)</p>
<!--
- Why did not the treatment work on the patient?
- Why was my loan rejected?
- Why have we not been contacted by alien life yet?
-->
<ul>
<li>どうして治療が患者に効かなかったのか？</li>
<li>どうして私のローン申請は却下されたのか？</li>
<li>どうして私たちはまだエイリアンからコンタクトを受けていないのか？</li>
</ul>
<!-- The first two questions can be answered with an "everyday"-explanation, while the third one comes from the category "More general scientific phenomena and philosophical questions". -->
<p>はじめの２つの疑問は「日常」の説明で答えることができる一方、３つ目の疑問は、より一般的な科学現象と哲学的疑問のカテゴリーから生じています。</p>
<!-- We focus on the "everyday"-type explanations, because those are relevant to interpretable machine learning.
Questions that start with "how" can usually be rephrased as "why" questions:
"How was my loan rejected?" can be turned into "Why was my loan rejected?". -->
<p>ここでは、解釈可能な機械学習に関係している「日常」タイプの質問に絞って考えます。 「どのように」で始まる疑問は、「なぜ」で始まる疑問に言い換えることができます。 例えば、「どのように私のローン申請は却下されましたか？は「なぜ私のローン申請は却下されたのですか？」に変換できます。</p>
<!-- In the following, the term "explanation" refers to the social and cognitive process of explaining, but also to the product of these processes.
The explainer can be a human being or a machine. -->
<p>以降では、「説明」という用語は説明の社会的、及び認知的なプロセスだけでなく、これらのプロセスの産物も含まれます。 説明をする人は、人間の場合も機械の場合もありえます。</p>
<!-- ### What Is a Good Explanation? {#good-explanation} -->
<!-- ### よい説明とは何か? {#good-explanation} -->
<!-- This section further condenses Miller's summary on "good" explanations and adds concrete implications for interpretable machine learning. -->
<p>このセクションでは、「良い」説明に関するMillerの要約をさらに凝縮し、解釈可能な機械学習に対する具体的な意味を付け加えます。</p>
<!-- **Explanations are contrastive** (Lipton 1990[^lipton2]).
Humans usually do not ask why a certain prediction was made, but why this prediction was made *instead of another prediction*. -->
<p><strong>説明は対照的です</strong>(Lipton 1990<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a>)。 人間は普段、ある予測をした理由について尋ねませんが、その予測が他の予測の代わりにされた理由を尋ねます。</p>
<!-- We tend to think in counterfactual cases, i.e. "How would the prediction have been if input X had been different?".
For a house price prediction, the house owner might be interested in why the predicted price was high compared to the lower price they had expected.
If my loan application is rejected, I do not care to hear all the factors that generally speak for or against a rejection. -->
<p>私たち人間は、事実に反する事例、つまり「もし入力Xが違っていたら、予測はどうなっていたか」を考える傾向があります。 例えば、住宅価格の予測については、住宅所有者は予測価格が予想よりも高かった場合、その理由に興味があるでしょう。 もしローン申請が却下されたなら、却下の原因に対する一般論ではなく、自分がローンを獲得するために変える必要がある申請の要素に興味があるでしょう。</p>
<!-- I am interested in the factors in my application that would need to change to get the loan.
I want to know the contrast between my application and the would-be-accepted version of my application.
The recognition that contrasting explanations matter is an important finding for explainable machine learning.
From most interpretable models, you can extract an explanation that implicitly contrasts a prediction of an instance with the prediction of an artificial data instance or an average of instances. -->
<p>ただ単に、却下された申請と、おそらく通るであろう申請の違いについて知りたいのです。 こうした対照的な説明が重要であるという認識は、説明可能な機械学習にとっても重要な発見です。 ほとんどの解釈可能なモデルは、１つのデータに対する予測と、人工的なデータもしくはデータの平均に対する予測とを暗黙的に対比する説明をするが出来ます。</p>
<!-- Physicians might ask: "Why did the drug not work for my patient?".
And they might want an explanation that contrasts their patient with a patient for whom the drug worked and who is similar to the non-responding patient.
Contrastive explanations are easier to understand than complete explanations. -->
<p>医師は、「どうして薬は患者に効かなかったのか」と尋ねるかもしれません。 この場合は、医師が必要とするのは、薬が効いた患者と、似た状況で薬が効かなかった患者との比較による説明です。 比較による説明は完全な説明よりも理解が簡単です。</p>
<!--
A complete explanation of the physician's question why the drug does not work might include:
The patient has had the disease for 10 years, 11 genes are over-expressed, the patients body is very quick in breaking the drug down into ineffective chemicals, ...
A contrastive explanation might be much simpler:
-->
<p>なぜ薬が効かないのかという医師の質問への完全な説明には次のようなものがあります。 「患者は10年間この病気にかかっていて、11の遺伝子が過剰発現しているため、体内で薬が非常に素早く効果のない化学物質に分解してしまい...」 比較による説明は、より簡潔です。</p>
<!-- In contrast to the responding patient, the non-responding patient has a certain combination of genes that make the drug less effective.
The best explanation is the one that highlights the greatest difference between the object of interest and the reference object.   -->
<p>「薬の効く患者と対照的に、薬の効かない患者は薬の効果を下げる特定の遺伝子の組み合わせを持っています。」 最良の説明というのは、興味のある対象と、参照する対象の最も大きな違いを強調できるものです。</p>
<!-- **What it means for interpretable machine learning**: -->
<p><strong>解釈可能な機械学習の意味</strong>:</p>
<!-- Humans do not want a complete explanation for a prediction, but want to compare what the differences were to another instance's prediction (can be an artificial one). -->
<p>人間は予測についての完璧な説明は求めませんが、違いが何であったかを他のインスタンスの予測（これは人工的でもかまいません）と比較したいと考えます。</p>
<!-- Creating contrastive explanations is application-dependent because it requires a point of reference for comparison.
And this may depend on the data point to be explained, but also on the user receiving the explanation.
A user of a house price prediction website might want to have an explanation of a house price prediction contrastive to their own house or maybe to another house on the website or maybe to an average house in the neighborhood. -->
<p>対照的な説明をするには、比較のためのデータ点が必要となるため、アプリケーションに依存します。また、これは説明されるデータ点だけでなく、説明を受けるユーザーにも依存する可能性があります。</p>
<!-- The solution for the automated creation of contrastive explanations might also involve finding prototypes or archetypes in the data. -->
<p>対照的な説明を自動的に生み出すための手法として、データ内にプロトタイプ、または、典型を見つける方法もあります。</p>
<!--
**Explanations are selected**.
-->
<p><strong>説明は選択的です</strong>。</p>
<!--
People do not expect explanations that cover the actual and complete list of causes of an event.
We are used to selecting one or two causes from a variety of possible causes as THE explanation.
As proof, turn on the TV news:
"The decline in stock prices is blamed on a growing backlash against the company's product due to problems with the latest software update."   
"Tsubasa and his team lost the match because of a weak defense: they gave their opponents too much room to play out their strategy."  
"The increasing distrust of established institutions and our government are the main factors that have reduced voter turnout."  
The fact that an event can be explained by various causes is called the Rashomon Effect.
Rashomon is a Japanese movie that tells alternative, contradictory stories (explanations) about the death of a samurai.
For machine learning models, it is advantageous if a good prediction can be made from different features.
Ensemble methods that combine multiple models with different features (different explanations) usually perform well because averaging over those "stories" makes the predictions more robust and accurate.
But it also means that there is more than one selective explanation why a certain prediction was made.  
**What it means for interpretable machine learning**:
Make the explanation very short, give only 1 to 3 reasons, even if the world is more complex.
The [LIME method](#lime) does a good job with this.
-->
<p>人々は、イベントの原因の完全なリストを網羅するような説明は期待していません。 むしろ人間は、考慮され得る様々な原因から1つまたは2つを説明として選択することに慣れています。 その証拠として、テレビニュースを考えてください。 「株価の下落は、最新のソフトウェアアップデートの問題によって同社の製品に対する反発が高まっていることが原因です。」 「Tsubasaと彼のチームはディフェンスが弱かったので試合に負けました。対戦相手が戦略を実行する余地を与え過ぎたのです。」 「国立機関と政府に対する不信の高まりが原因で、投票率が下落しました。」 ある出来事がさまざまな原因で説明できることを羅生門効果と呼びます。 羅生門は、武士の死について別の矛盾した物語（説明）を伝える日本の映画です。 機械学習モデルの場合、別の特徴量を用いたとしても適切な予測ができれば有利です。 異なる特徴（異なる説明）を持つ複数のモデルを組み合わせるアンサンブル学習は、これらの「物語」を平均化してよりロバストかつ正確に予測するため、ほとんどの場合で性能が よくなります。しかし、それは特定の予測が行われた理由の説明となる選択肢が複数あることも意味してます。 <strong>解釈可能な機械学習に対する意味</strong>： 世界がより複雑であっても、1つから3つの理由だけを用いて端的な説明をしてください。 [LIME]（＃lime）という手法を用いると、上記のことができます。</p>
<!--
**Explanations are social**.
They are part of a conversation or interaction between the explainer and the receiver of the explanation.
The social context determines the content and nature of the explanations.
If I wanted to explain to a technical person why digital cryptocurrencies are worth so much, I would say things like:
"The decentralized, distributed, blockchain-based ledger, which cannot be controlled by a central entity, resonates with people who want to secure their wealth, which explains the high demand and price."
But to my grandmother I would say:
"Look, Grandma: Cryptocurrencies are a bit like computer gold. People like and pay a lot for gold, and young people like and pay a lot for computer gold."  
**What it means for interpretable machine learning**:
Pay attention to the social environment of your machine learning application and the target audience.
Getting the social part of the machine learning model right depends entirely on your specific application.
Find experts from the humanities (e.g. psychologists and sociologists) to help you.
-->
<p><strong>説明は社会的です</strong>。 説明は説明する人と説明を受ける人との会話や相互作用の一部です。 社会的な文脈が説明の内容と性質を決定します。 デジタル暗号通貨がなぜそれほど価値があるのか​​を技術者に説明したい場合、次のように説明するでしょう： 「中央のエンティティでは制御できない非中央集権的な分散型ブロックチェーンを基盤とした台帳は、富を安全に保持したい人々の共感を呼んでいます。これによって、需要と価格が高くなるのです。」 しかし、祖母に説明する場合には： 「ほら、おばあちゃん：暗号通貨はコンピューター上の金のようなものなんだ。みんな金が好きで、金のためにお金を払うけれど、若い人はコンピュータ上の金が好きでお金を払っているんだ。」 <strong>解釈可能な機械学習に対する意味</strong>： 機械学習アプリケーションの社会的な位置付けと対象とする利用者に注意を払ってください。 機械学習モデルの社会的な部分を正しく把握することは、アプリケーションに完全に依存します。 人文科学の専門家（心理学者や社会学者など）を見つけて支援を受けてください。</p>
<!--
**Explanations focus on the abnormal**.
People focus more on abnormal causes to explain events (Kahnemann and Tversky, 1981[^Kahnemann]).
These are causes that had a small probability but nevertheless happened.
The elimination of these abnormal causes would have greatly changed the outcome (counterfactual explanation).
Humans consider these kinds of "abnormal" causes as good explanations.
An example from Štrumbelj and Kononenko (2011)[^Strumbelj2011] is:
Assume we have a dataset of test situations between teachers and students.
Students attend a course and pass the course directly after successfully giving a presentation.
The teacher has the option to additionally ask the student questions to test their knowledge.
Students who cannot answer these questions will fail the course.
Students can have different levels of preparation, which translates into different probabilities for correctly answering the teacher's questions (if they decide to test the student).
We want to predict whether a student will pass the course and explain our prediction.
The chance of passing is 100% if the teacher does not ask any additional questions, otherwise the probability of passing depends on the student's level of preparation and the resulting probability of answering the questions correctly.  
Scenario 1:
The teacher usually asks the students additional questions (e.g. 95 out of 100 times).
A student who did not study (10% chance to pass the question part) was not one of the lucky ones and gets additional questions that he fails to answer correctly.
Why did the student fail the course?
I would say that it was the student's fault to not study.  
Scenario 2:
The teacher rarely asks additional questions (e.g. 2 out of 100 times).
For a student who has not studied for the questions, we would predict a high probability of passing the course because questions are unlikely.
Of course, one of the students did not prepare for the questions, which gives him a 10% chance of passing the questions.
He is unlucky and the teacher asks additional questions that the student cannot answer and he fails the course.
What is the reason for the failure?
I would argue that now, the better explanation is "because the teacher tested the student".
It was unlikely that the teacher would test, so the teacher behaved abnormally.  
**What it means for interpretable machine learning**:
If one of the input features for a prediction was abnormal in any sense (like a rare category of a categorical feature) and the feature influenced the prediction, it should be included in an explanation, even if other 'normal' features have the same influence on the prediction as the abnormal one.
An abnormal feature in our house price prediction example might be that a rather expensive house has two balconies.
Even if some attribution method finds that the two balconies contribute as much to the price difference as the above average house size, the good neighborhood or the recent renovation, the abnormal feature "two balconies" might be the best explanation for why the house is so expensive.
-->
<p><strong>異常に焦点を当てた説明</strong> 人々は出来事を説明するために、より正常でない原因に焦点を当てます (Kahnemann and Tversky, 1981<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a>)。 これらは、確率が低いにも関わらず発生してしまった原因です。 これらの正常でない原因を排除することで、結果は大きく変わるでしょう（反事実的説明）。 人々はこれらの「異常な」原因を良い説明とみなします。 Štrumbelj と Kononenko (2011)<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a>の次のような例があります： 先生と生徒間のテストに関するデータセットがあると仮定します。 生徒は授業に出席し、プレゼンテーションに成功するとその授業に合格できます。 先生は、生徒の知識をテストする試験を追加で行う選択肢があります。 これら試験の質問に答えられない生徒はその授業に落第します。 生徒はさまざまなレベルで準備できます。これは、先生の試験に正しく答えるための様々な確率へと変換できます（テストを実施する場合）。 生徒が授業に合格するかどうかを予測し、その予測の理由も説明してみましょう。 先生が追加の試験を実施しなければ合格率は100％です。そうでなければ、合格率は生徒の準備レベルと質問に正しく答える結果の確率に依存します。 シナリオ1： 先生は、100回のうち95回というように、常に生徒に対して追加の試験を実施するとしましょう。 （質問の部分に合格する可能性が10％の）勉強しなかった生徒は、幸運な生徒ではなく、正解できない追加の試験を受けます。 なぜ生徒は授業に落第したのでしょうか？ それは、生徒が勉強しなかったからです。 シナリオ2： 先生は、100回のうち2回というように、稀にしか試験を実施しないとしましょう。 試験の勉強をしていない生徒でも、試験が実施される可能性が低いため、授業に合格する可能性が高いと予測されます。 ある生徒は試験の準備をしていないので、10％の確率でしか試験に合格できません。 そして、不運にも、先生は彼が答えられない追加の試験を実施したので、授業に落第しました。 彼はなぜ落第してしまったのでしょうか？ この場合、より適した説明は「先生が生徒に対して試験を実施したから」だと言えます。 先生が試験を実施する可能性は低いので、先生は普通ではない行動をしたと言えます。 <strong>解釈可能な機械学習に対する意味</strong>： 予測の入力特徴の1つが（カテゴリ特徴のうち、ごく稀なカテゴリなど）何らかの意味で異常であり、その特徴が予測に影響を与えた場合、たとえ他の「正常な」特徴が予測に対して同じ影響を与えたとしても、説明に含めるべきです。 住宅価格予測の例の異常な特徴として、極めて高価な家には2つのバルコニーがあることかもしれません。 2つのバルコニーがあるということが、いくつかの方法で、家の大きさ、良好な近隣住民、または最近リフォームされたことが同じくらい価格差に寄与していることがわかったとしても、異常な特徴である「2つのバルコニー」という特徴は、その家が高価格となっている理由の最良の説明となるでしょう。</p>
<!--
**Explanations are truthful**.
Good explanations prove to be true in reality (i.e. in other situations).
But disturbingly, this is not the most important factor for a "good" explanation.
For example, selectiveness seems to be more important than truthfulness.
An explanation that selects only one or two possible causes rarely covers the entire list of relevant causes.
Selectivity omits part of the truth.
It is not true that only one or two factors, for example, have caused a stock market crash, but the truth is that there are millions of causes that influence millions of people to act in such a way that in the end a crash was caused.  
**What it means for interpretable machine learning**:
The explanation should predict the event as truthfully as possible, which in machine learning is sometimes called **fidelity**.
So if we say that a second balcony increases the price of a house, then that also should apply to other houses (or at least to similar houses).
For humans, fidelity of an explanation is not as important as its selectivity, its contrast and its social aspect.
-->
<p><strong>説明は真実</strong>。 適切な説明は実際に（他の状況で）真実であることを証明します。 しかし気がかりなことに、これは「適切な」説明にとって最も重要な要素ではありません。 例えば、選択性は真実性よりも重要であると考えられます。 たった1つか2つの原因のみによる説明が、関連する原因全てを網羅することはめったにありません。選択性は一部の真実を省略します。 例として、1つか2つの要因だけで株式市場の暴落を引き起こすことはありません。真実としては、何百万もの人々が最終的に暴落を引き起こす行動をとるように影響を与えた何百万もの原因があるのです。 <strong>解釈可能な機械学習に対する意味</strong>： 説明は出来事をできるだけ正確に説明する必要があります。これは、機械学習において<strong>忠実性 (fidelity)</strong>と呼ばれることがあります。したがって、2つのバルコニーが家の価格を上げると主張する場合、それは他の家（あるいは少なくとも同じような家）にも当てはまるべきです。 人間にとって、説明の忠実性は、その選択性、対照性、社会的側面ほど重要ではありません。</p>
<!--
**Good explanations are consistent with prior beliefs of the explainee**.
Humans tend to ignore information that is inconsistent with their prior beliefs.
This effect is called confirmation bias (Nickerson 1998[^Nickerson]).
Explanations are not spared by this kind of bias.
People will tend to devalue or ignore explanations that do not agree with their beliefs.
The set of beliefs varies from person to person, but there are also group-based prior beliefs such as political worldviews.  
**What it means for interpretable machine learning**:
Good explanations are consistent with prior beliefs.
This is difficult to integrate into machine learning and would probably drastically compromise predictive performance.
Our prior belief for the effect of house size on predicted price is that the larger the house, the higher the price.
Let us assume that a model also shows a negative effect of house size on the predicted price for a few houses.
The model has learned this because it improves predictive performance (due to some complex interactions), but this behavior strongly contradicts our prior beliefs.
You can enforce monotonicity constraints (a feature can only affect the prediction in one direction) or use something like a linear model that has this property.
-->
<p><strong>よい説明は、説明対象者の事前の信念と一貫している</strong>。 人間は、自分が信じていることと矛盾する情報を無視する傾向があります。 この効果は確証バイアスと呼ばれます (Nickerson 1998<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a>)。 説明はこの種の偏見から免れることができません。 人間は自分の信念と矛盾する説明を軽んじたり無視したりする傾向があります。 信念は人によって異なりますが、政治的世界観など集団に基づくものもあります。 <strong>解釈可能な機械学習に対する意味</strong>： よい説明は人間が信じていることと一貫しています。 これを機械学習に統合するのは難しく、統合できたとしても、おそらく予測性能を大幅に損なうことになります。 家の大きさが予測価格に与える影響について、我々は家が大きいほど価格が高くなると信じています。 いくつかの家の予測価格に対して、家のサイズがマイナスの影響を与えていたとモデルが示していたとします。 モデルは予測性能を向上させるために、（いくつかの複雑な相互作用の影響によって）このように学習しましたが、この振る舞いは我々が信じていることと強く矛盾しています。 （特徴量が一方向の予測にのみ影響を与えるような）単調性制約を適用するか、この性質を持つ線形モデルなどを使用できます。</p>
<!--
**Good explanations are general and probable**.
A cause that can explain many events is very general and could be considered a good explanation.
Note that this contradicts the claim that abnormal causes make good explanations.
As I see it, abnormal causes beat general causes.
Abnormal causes are by definition rare in the given scenario.
In the absence of an abnormal event, a general explanation is considered a good explanation.
Also remember that people tend to misjudge probabilities of joint events.
(Joe is a librarian. Is he more likely to be a shy person or to be a shy person who likes to read books?)
A good example is "The house is expensive because it is big", which is a very general, good explanation of why houses are expensive or cheap.  
**What it means for interpretable machine learning**:
Generality can easily be measured by the feature's support, which is the number of instances to which the explanation applies divided by the total number of instances.
-->
<p><strong>適切な説明は一般的でもっともらしいものである</strong> 多くの出来事を説明できる原因は非常に一般的であり、よい説明と見なされます。 ただし、これは普通でない原因がよい説明であるという主張と矛盾していることに注意してください。 著者の考えでは、異常な原因による説明は一般的な原因による説明を上回っています。 異常な原因は定義上、特定の文脈や状況において稀なものです。 異常な出来事がない場合、一般的な説明はよい説明であると見なされます。 人間は同時に起こる出来事の確率を誤解する傾向があることを忘れないでください。 （Joeは司書です。彼は恥ずかしがり屋である可能性が高いですか、それとも本を読むのが好きな恥ずかしがり屋である可能性が高いですか？） 「家が大きいのでその価格は高い」というのは分かりやすい例です。これは非常に一般的で、家が高いあるいは安い理由をよく説明しています。 <strong>解釈可能な機械学習に対する意味</strong>： 一般性は、特徴量のサポートによって簡単に測定できます。これは、説明が適用されるインスタンスの数をインスタンスの総数で割ったものです。</p>

<!--
# Datasets {#data}
-->
</div>
</div>
<!-- </div> -->
<div class="footnotes">
<hr />
<ol start="10">
<li id="fn10"><p>Lipton, Peter. &quot;Contrastive explanation.&quot; Royal Institute of Philosophy Supplements 27 (1990): 247-266.<a href="explanation.html#fnref10">↩</a></p></li>
<li id="fn11"><p>Kahneman, Daniel, and Amos Tversky. &quot;The Simulation Heuristic.&quot; Stanford Univ CA Dept of Psychology. (1981).<a href="explanation.html#fnref11">↩</a></p></li>
<li id="fn12"><p>Štrumbelj, Erik, and Igor Kononenko. &quot;A general method for visualizing and explaining black-box regression models.&quot; In International Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer. (2011).<a href="explanation.html#fnref12">↩</a></p></li>
<li id="fn13"><p>Nickerson, Raymond S. &quot;Confirmation Bias: A ubiquitous phenomenon in many guises.&quot; Review of General Psychology 2 (2). Educational Publishing Foundation: 175. (1998).<a href="explanation.html#fnref13">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="properties.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/interpretable-ml-book/edit/master/02-interpretability.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
